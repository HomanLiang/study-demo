[toc]



# ElasticSearch 面试题



## REST API在 Elasticsearch 方面有哪些优势？

REST API是使用超文本传输协议的系统之间的通信，该协议以 XML 和 JSON格式传输数据请求。

REST 协议是无状态的，并且与带有服务器和存储数据的用户界面分开，从而增强了用户界面与任何类型平台的可移植性。它还提高了可伸缩性，允许独立实现组件，因此应用程序变得更加灵活。

REST API与平台和语言无关，只是用于数据交换的语言是XML或JSON。

借助：REST API 查看集群信息或者排查问题都非常方便。



## Elasticsearch的倒排索引是什么？

面试官：想了解你对基础概念的认知。

解答：通俗解释一下就可以。

倒排索引是搜索引擎的核心。搜索引擎的主要目标是在查找发生搜索条件的文档时提供快速搜索。倒排索引是一种像数据结构一样的散列图，可将用户从单词导向文档或网页。它是搜索引擎的核心。其主要目标是快速搜索从数百万文件中查找数据。

传统的我们的检索是通过文章，逐个遍历找到对应关键词的位置。

而倒排索引，是通过分词策略，形成了词和文章的映射关系表，这种词典+映射表即为倒排索引。

有了倒排索引，就能实现o（1）时间复杂度的效率检索文章了，极大的提高了检索效率。

![image-20210227222928353](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/elastic-search-demo/image-20210227222928353.png)

学术的解答方式：

倒排索引，相反于一篇文章包含了哪些词，它从词出发，记载了这个词在哪些文档中出现过，由两部分组成——词典和倒排表。

**加分项**：倒排索引的底层实现是基于：FST（Finite State Transducer）数据结构。

lucene从4+版本后开始大量使用的数据结构是FST。FST有两个优点：

1. 空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；
2. 查询速度快。O(len(str))的查询时间复杂度。

 

## 解释 Elasticsearch 中的相关性和得分？

当你在互联网上搜索有关 Apple 的信息时。它可以显示有关水果或苹果公司名称的搜索结果。

- 你可能要在线购买水果，检查水果中的食谱或食用水果，苹果对健康的好处。
- 你也可能要检查Apple.com，以查找该公司提供的最新产品范围，检查评估公司的股价以及最近6个月，1或5年内该公司在纳斯达克的表现。

同样，当我们从 Elasticsearch 中搜索文档（记录）时，你会对获取所需的相关信息感兴趣。基于相关性，通过Lucene评分算法计算获得相关信息的概率。

ES 会将相关的内容都返回给你，只是：计算得出的评分高的排在前面，评分低的排在后面。

计算评分相关的两个核心因素是：词频和逆向文档频率（文档的稀缺性）。

大体可以解释为：单篇文档词频越高、得分越高；多篇文档某词越稀缺，得分越高。



## 请解释有关 Elasticsearch的 NRT？

从文档索引（写入）到可搜索到之间的延迟默认一秒钟，因此Elasticsearch是近实时（NRT）搜索平台。

也就是说：文档写入，最快一秒钟被索引到，不能再快了。

写入调优的时候，我们通常会动态调整：refresh_interval = 30s 或者更达值，以使得写入数据更晚一点时间被搜索到。





## elasticsearch 是如何实现 master 选举的

面试官：想了解 ES 集群的底层原理，不再只关注业务层面了。

前置前提：

1. 只有候选主节点（master：true）的节点才能成为主节点。
2. 最小主节点数（min_master_nodes）的目的是防止脑裂。

Elasticsearch 的选主是 ZenDiscovery 模块负责的，主要包含 Ping（节点之间通过这个RPC来发现彼此）和 Unicast（单播模块包含一个主机列表以控制哪些节点需要 ping 通）这两部分；
获取主节点的核心入口为 findMaster，选择主节点成功返回对应 Master，否则返回 null。

选举流程大致描述如下：
第一步：确认候选主节点数达标，elasticsearch.yml 设置的值 discovery.zen.minimum_master_nodes;
第二步：对所有候选主节点根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。
第三步：如果对某个节点的投票数达到一定的值（候选主节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。

- 补充：
  - 这里的 id 为 string 类型。
  - master 节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data 节点可以关闭 http 功能







## Elasticsearch 是如何实现 Master 选举的？

（1）Elasticsearch 的选主是 ZenDiscovery 模块负责的，主要包含 Ping（节点之间通过这个 RPC 来发现彼此）和 Unicast（单播模块包含一个主机列表以控制哪些节点需要 ping 通）这两部分；

（2）对所有可以成为 master 的节点（node.master: true）根据 nodeId 字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第 0 位）节点，暂且认为它是 master 节点。

（3）如果对某个节点的投票数达到一定的值（可以成为 master 节点数 n/2+1）并且该节点自己也选举自己，那这个节点就是 master。否则重新选举一直到满足上述条件。

（4）补充：master 节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data 节点可以关闭 http 功能。



## Elasticsearch 中的节点（比如共 20 个），其中的 10 个选了一个 master，另外 10 个选了另一个 master，怎么办？

1. 当集群 master 候选数量不小于 3 个时，可以通过设置最少投票通过数量（discovery.zen.minimum_master_nodes）超过所有候选节点一半以上来解决脑裂问题；
2. 当候选数量为两个时，只能修改为唯一的一个 master 候选，其他作为 data节点，避免脑裂问题。



## 如何解决ES集群的脑裂问题

所谓集群脑裂，是指 Elasticsearch 集群中的节点（比如共 20 个），其中的 10 个选了一个 master，另外 10 个选了另一个 master 的情况。

当集群 master 候选数量不小于 3 个时，可以通过设置最少投票通过数量（discovery.zen.minimum_master_nodes）超过所有候选节点一半以上来解决脑裂问题；当候选数量为两个时，只能修改为唯一的一个 master 候选，其他作为 data 节点，避免脑裂问题。



## 在并发情况下，ES如果保证读写一致？

可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；
另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。
对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。



### ES对于大数据量（上亿量级）的聚合如何实现？

Elasticsearch 提供的首个近似聚合是cardinality 度量。它提供一个字段的基数，即该字段的distinct或者unique值的数目。它是基于HLL算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关



## 对于GC方面，在使用ES时要注意什么？

1. 倒排词典的索引需要常驻内存，无法GC，需要监控data node上segment memory增长趋势。
1. 各类缓存，field cache, filter cache, indexing cache, bulk queue等等，要设置合理的大小，并且要应该根据最坏的情况来看heap是否够用，也就是各类缓存全部占满的时候，还有heap空间可以分配给其他任务吗？避免采用clear cache等“自欺欺人”的方式来释放内存。
1. 避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用scan & scroll api来实现。
1. cluster stats驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过tribe node连接。
1. 想知道heap够不够，必须结合实际应用场景，并对集群的heap使用情况做持续的监控。



## Elasticsearch作为解决方案需要注意什么？

本文以15年国外经典博客的框架为线索，剔除过时的技术体系、技术栈内容，结合近千万级业务场景和最新Elastic技术洞察重新梳理出：Elasticsearch方案选型必须了解的10件事。

### 集群规模

Elasticsearch的优点在于它是非常容易扩展。但，索引和查询时间可能因许多因素而异。在集群规模层面一方面要考虑数据量，另一方面比较重要的衡量因素是项目/产品的指标要求。

要想达到吞吐量和CPU利用率的指标要求，建议进行一定量的测试，以确认集群承担的负载和性能瓶颈问题。

测试工具推荐：`Apache Jmeter`。

网上会有很多的一线互联网公司等的“他山之石”，但，方案仅供参考，需要自己结合业务场景、硬件资源进行`反复测试`验证。



### 节点职责

Elasticsearch节点可以是主节点（Master），数据节点（Data），客户端/路由节点（Client）或某种组合。 大多数人大规模集群选择专用主节点（至少3个），然后选择一些数据和客户端节点。

建议：`职责分离`，并您针对特定工作负载优化每种类型的节点的分配。

例如，通过分离客户端和数据节点提升性能。 客户端节点处理传入的HTTP请求，这使得数据节点为查询提供服务。

这并不是绝对的，有大量网友在社区反馈，分离客户端节点并没有提升性能，因实际场景而异，大规模数据增量的业务场景，职责分离必然是大势所趋。



### 安全

近期，未加任何安全防护措施的Elastic`安全事件`频发。建议在应用程序API和Elasticsearch层之间以及Elasticsearch层和内部网络之间保护您的Elasticsearch集群。

1. 6.3+版本之后，xpack插件已经集成到Elastic产品线。（收费）
2. 加一层Nginx代理，能防止未经授权的访问。
3. 其他选型推荐：search-guard，readonlyRest等。

“裸奔的风险非常大”，进阶阅读：[你的Elasticsearch在裸奔吗？](http://mp.weixin.qq.com/s?__biz=MzI2NDY1MTA3OQ==&mid=2247484309&idx=1&sn=0f3921611ea97715cf616d2c13ba85a2&chksm=eaa82bbddddfa2aba7a54203a2a6a2a1041ef1ba3ed19dd24d953b526cab4b21539a04e4159e&scene=21#wechat_redirect)



### 数据建模

#### 使用别名

业务层面使用`别名`进行检索、聚合操作。

别名的好处：

1. 将应用和索引名称隔离；
2. 可以方便的实现跨索引检索。



#### 数据类型选型

若不指定数据类型的动态映射机制，比如：字符串类型会默认存储为text和keyword两种类型，势必会`增加存储成本`。
建议：针对业务场景需求，静态的手动指定好每个字段的数据类型。

考虑因素包含但不限于：
1、是否需要索引；
2、是否需要存储；
3、是否需要分词；
4、是否需要聚合；
5、是否需要多表关联（nested类型、join或者是宽表存储）；
6、是否需要快速响应（keyword和long类型选型）
……
此处的`设计时间不能省`。



### 检索选型

Elasticsearch查询DSL非常庞大。如果业务场景不需要计算评分，推荐使用过滤器`filter`。因为基于缓存，更高效。
查询相关的API包含但不限于：

- match/multi_match
- match_phrase/match_phrase_prefix
- term/terms
- wildcard/regexp
- query_string

选型前，建议通过Demo验证一下是否符合预期。

了解如何编写高效查询是一回事，但让它们返回最终用户期望的结果是另一回事。

业务实战中，建议`花一些时间`调整分析器、分词和评分，以便ES返回期望的正确的命中。



### 监控和警报

请务必考虑一个完全独立的“监视”集群机制，该机制仅用于捕获有关群集运行状况的统计信息，并在出现问题时提醒您。

**监控作用**：能通过可视化的方式，直观的看到内存、JVM、CPU、负载、磁盘等的使用情况，以对可能的突发情况及早做出应对方案。

**警报作用**：异常实时预警。

ES6.X xpack已经集成watcher工具。它会监视某些条件，并在满足这些条件时提醒您。

举例：当某些状态（例如JVM堆）达到阈值时，您可以采取一些操作（发送电子邮件，调用Web钩子等）。

如果你的业务场景是：几乎实时地将数据写入Elasticsearch并希望在数据与某些`模式匹配`时收到警报，则推荐使用`ElastAlert`。

https://github.com/Yelp/elastalert



### 节点配置和配置管理

一旦拥有多个节点，就每个节点在软件版本、配置等方面`保持同步`变得具有挑战性。

有许多开源工具可以帮助解决这个问题。推荐：`Chef`和`Ansible`帮助管理Elasticsearch集群。

`Ansible`可以自动执行升级和配置传播，而无需在任何Elasticsearch节点上安装任何其他软件。

当前可能看不到对自动化的巨大需求，如果要从小规模开始发展，并且希望能够快速发展的话，一个使用Ansible编写的常见任务库可以使你在几分钟内从裸服务器转到完全配置的Elasticsearch节点，`无需人工干预`。

增量索引的管理推荐：rollover + curator + crontab，6.6版本的新特性：`Index Lifecycle Management(索引生命周期管理）`，推荐尝鲜使用。



### 备份和恢复

经常被问到的问题1“ES中误删除的数据（delete或者delete_by_query）能恢复吗？”
——答案：如果做了备份，是可以的。如果没有，不可以。

问题2：“迁移节点，直接data路径原封不动拷贝可以吗？”
——答案：不可以，不推荐。推荐使用reindex或其他工具实现。

对于高可用性的业务系统，数据的`备份`功能非常重要。 由于数据的存储可能会涉及多个节点，依赖OS级文件系统备份可能会很冒险。

推荐使用Elasticsearch内置的“`快照`”功能，可以备份您的索引。



### API选型

Elastic`官方`支持API，包含：JAVA、Java Script、.net、PHP、python、Ruby。
Elastic民间API（社区贡献）非常庞大：C++、Go等20多种。

API选型推荐使用：`官方API`。

原因：
1）版本更新及时、
2）新特性支持适配更新及时。

http://t.cn/EMUzubT

http://t.cn/EMUzubH

DSL开发推荐使用的Kibana的`Dev-tool`，非常高效、方便。



### 数据接入

将数据索引到Elasticsearch很容易。 根据数据源和其他因素，您可以自己编写，也可以使用Elastic中的`Logstash`工具。

Logstash可以查看日志文件或其他输入，然后有效地将数据索引到集群中。

其他大数据组件或开源项目也有类似的功能，举例：

`kafka-connector`，`flume`，`canal`等。

选型中，`不一棵树上吊死`，综合对比性能和稳定性，找适合自己业务场景的最为重要。



### 小结

安装和运行开箱即用的Elasticsearch集群非常简单。 使其适用于你的实际业务场景并满足你的性能指标非常不容易。





## es分布式架构原理

首先需要明白es是如何存储数据的，es把对应的数据转换为index。基于倒排索引的方式，每个index上存储了多个type类型，每个type对应一个document。而一个index会被分成多个shard(默认是5个)。 在分布式部署时，每个shard会被复制，即一个shard有primary和replica 每个es进程存储的是不同shard的primary和replica。es集群多个节点，会自动选举一个节点为master节点，这个master节点其实就是干一些管理的工作的，比如维护索引元数据，负责切换primary shard和replica shard身份。 

![image-20210227231935464](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/elastic-search-demo/image-20210227231935464.png)



## es的数据写入与读取

### es数据的写入

#### es数据的写入过程

注意，客户端是可以在任意节点进行写入数据的，与Kakfa不同。 

1. 客户端选择一个node发送请求过去，这个node就是coordinating node（协调节点）
2. coordinating node，对document进行路由得到对应应该存储到哪个shard，将请求转发给对应的node（有primary shard） 
3. 实际的node上的primary shard处理请求，然后将数据同步到replica node 
4. coordinating node，如果发现primary node和所有replica node都搞定之后，就返回响应结果给客户端 

![image-20210227232231301](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/elastic-search-demo/image-20210227232231301.png)



#### es数据的写入原理

es数据写入原理主要可以分为4个操作：

1. refresh
2. commit
3. flush
4. merge

|             |                         操作触发条件                         | 操作过程                                                     |
| ----------- | :----------------------------------------------------------: | ------------------------------------------------------------ |
| **refresh** | 1. 每隔1s进行一次refresh操作 2. buffer已满，则进行一次refresh操作 | 1. buffer将数据写入segment file 2. 清空buffer                |
| **commit**  |      1. 每隔30分钟执行一次translog 2. translog日志已满       | 1. 会主动进行一次refresh操作，把buffer中的数据写入到segment file 2. 生成一个 commit point 文件标识此次操作一件把buffer数据执行到了哪一个segment文件 3. 执行flush操作 |
| **flush**   |                         commit操作中                         | 1. 把file system上的文件全部强制fsync（持久化）到磁盘 2. 清空translog文件 3. 生成一个新的translog文件 |
| **merge**   |                           后台检查                           | 1. 将多个segment文件合并为一个文件，并把.del文件删除 2. commit log 更新标识目前的segment 3. 打开segmentfile 到file cache 以供快速搜索 4. 删除旧的segment file |

<img src="https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/elastic-search-demo/up-93210c6ea1f83b2d638ee38eb5b1af2c35c.png" alt="up-93210c6ea1f83b2d638ee38eb5b1af2c35c"  />



### es数据的读取

#### 读取数据

使用RestFul API向对应的node发送查询请求，根据did来判断在哪个shard上，返回的是primary和replica的node节点集合 这样会负载均衡地把查询发送到对应节点，之后对应节点接收到请求，将document数据返回协调节点，协调节点把document返回给客户端 

![up-d7f4d579ea5edca6214b27b8b84e3ae2526](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/elastic-search-demo/up-d7f4d579ea5edca6214b27b8b84e3ae2526.png)

#### 全文检索

1. 客户端使用RestFul API向对应的node发送查询请求
1. 协调节点将请求转发到所有节点（primary或者replica）所有节点将对应的数据查询之后返回对应的doc id 返回给协调节点
1. 协调节点将doc进行排序聚合
1. 协调节点再根据doc id 把查询请求发送到对应shard的node，返回document





## 详细描述一下 Elasticsearch 写入索引文档的过程

面试官：想了解 ES 的底层原理，不再只关注业务层面了。

解答：

这里的索引文档应该理解为文档写入 ES，创建索引的过程。

文档写入包含：单文档写入和批量 bulk 写入，这里只解释一下：单文档写入流程。

记住官方文档中的这个图。

![image-20210228001106769](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/elastic-search-demo/image-20210228001106769.png)

第一步：客户写集群某节点写入数据，发送请求。（如果没有指定路由/协调节点，请求的节点扮演路由节点的角色。）

第二步：节点 1 接受到请求后，使用文档_id 来确定文档属于分片 0。请求会被转到另外的节点，假定节点 3。因此分片 0 的主分片分配到节点 3 上。

第三步：节点 3 在主分片上执行写操作，如果成功，则将请求并行转发到节点 1和节点 2 的副本分片上，等待结果返回。所有的副本分片都报告成功，节点 3 将向协调节点（节点 1）报告成功，节点 1 向请求客户端报告写入成功。

如果面试官再问：第二步中的文档获取分片的过程？

回答：借助路由算法获取，路由算法就是根据路由和文档 id 计算目标的分片 id 的过程。

```bash
1shard = hash(_routing) % (num_of_primary_shards)
```



## 详细描述一下 Elasticsearch 搜索的过程？

面试官：想了解 ES 搜索的底层原理，不再只关注业务层面了。

解答：

搜索拆解为“query then fetch” 两个阶段。

query 阶段的目的：定位到位置，但不取。

步骤拆解如下：

（1）假设一个索引数据有 5 主+1 副本 共 10 分片，一次请求会命中（主或者副本分片中）的一个。

（2）每个分片在本地进行查询，结果返回到本地有序的优先队列中。

（3）第 2）步骤的结果发送到协调节点，协调节点产生一个全局的排序列表。

fetch 阶段的目的：取数据。

路由节点获取所有文档，返回给客户端。







## es在生产集群的部署架构是什么，每个索引有多大的数据量，每个索引有多少分片

生产环境部署情况 :

1. es生产集群我们部署了5台机器，每台机器是6核64G的，集群总内存是320G
2. 我们es集群的日增量数据大概是2000万条，每天日增量数据大概是500MB， 每月增量数据大概是6亿，15G。目前系统已经运行了几个月，现在es集群里数据总量大概是100G左右。
3. 目前线上有5个索引（这个结合你们自己业务来，看看自己有哪些数据可以放es的）， 每个索引的数据量大概是20G，所以这个数据量之内，我们每个索引分配的是8个shard，比默认的5个shard多了3个shard。



## elasticsearch 了解多少，说说你们公司 es 的集群架构，索引数据大小，分片有多少，以及一些调优手段 。

面试官：想了解应聘者之前公司接触的 ES 使用场景、规模，有没有做过比较大规模的索引设计、规划、调优。

解答：如实结合自己的实践场景回答即可。

比如：ES 集群架构 13 个节点，索引根据通道不同共 20+索引，根据日期，每日递增 20+，索引：10 分片，每日递增 1 亿+数据，每个通道每天索引大小控制：150GB 之内。

我司有多个ES集群，下面列举其中一个。该集群有20个节点，根据数据类型和日期分库，每个索引根据数据量分片，比如日均1亿+数据的，控制单索引大小在200GB以内。　

仅索引层面调优手段：

### 设计阶段调优

1. 根据业务增量需求，采取基于日期模板创建索引，通过 roll over API 滚动索引；
2. 使用别名进行索引管理；
3. 每天凌晨定时对索引做 force_merge 操作，以释放空间；
4. 采取冷热分离机制，热数据存储到 SSD，提高检索效率；冷数据定期进行 shrink操作，以缩减存储；
5. 采取 curator 进行索引的生命周期管理；
6. 仅针对需要分词的字段，合理的设置分词器；
7. Mapping 阶段充分结合各个字段的属性，是否需要检索、是否需要存储等。……..



### 写入调优

1. 写入前副本数设置为 0；
2. 写入前关闭 refresh_interval 设置为-1，禁用刷新机制；
3. 写入过程中：采取 bulk 批量写入；
4. 写入后恢复副本数和刷新间隔；
5. 尽量使用自动生成的 id。



### 查询调优

1. 禁用 wildcard；
2. 禁用批量 terms（成百上千的场景）；
3. 充分利用倒排索引机制，能 keyword 类型尽量 keyword；
4. 数据量大时候，可以先基于时间敲定索引再检索；
5. 设置合理的路由机制。



### 其他调优

部署调优，业务调优等。

上面的提及一部分，面试者就基本对你之前的实践或者运维经验有所评估了。





## elasticsearch 索引数据多了怎么办，如何调优，部署

面试官：想了解大数据量的运维能力。

解答：索引数据的规划，应在前期做好规划，正所谓“设计先行，编码在后”，这样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。

如何调优，正如问题 1 所说，这里细化一下：

### 索引层面：

1）使用批量请求并调整其大小：每次批量数据 5–15 MB 大是个不错的起始点。
2）段合并：Elasticsearch默认值是20MB/s，对机械磁盘应该是个不错的设置。如果你用的是SSD，可以考虑提高到100-200MB/s。如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。另外还可以增加 index.translog.flush_threshold_size 设置，从默认的512MB到更大一些的值，比如1GB，这可以在一次清空触发的时候在事务日志里积累出更大的段。
3）如果你的搜索结果不需要近实时的准确度，考虑把每个索引的index.refresh_interval 改到30s。
4）如果你在做大批量导入，考虑通过设置index.number_of_replicas: 0 关闭副本。
5）需要大量拉取数据的场景，可以采用scan & scroll api来实现，而不是from/size一个大范围。

### 动态索引层面

基于模板+时间+rollover api 滚动创建索引，举例：设计阶段定义：blog 索引的模板格式为：blog_index_时间戳的形式，每天递增数据。这样做的好处：不至于数据量激增导致单个索引数据量非常大，接近于上线 2 的32 次幂-1，索引存储达到了 TB+甚至更大。

一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。



### 存储层面

冷热数据分离存储，热数据（比如最近 3 天或者一周的数据），其余为冷数据。

对于冷数据不会再写入新数据，可以考虑定期 force_merge 加 shrink 压缩操作，节省存储空间和检索效率。

1）基于数据+时间滚动创建索引，每天递增数据。控制单个索引的量，一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。
2）冷热数据分离存储，热数据（比如最近3天或者一周的数据），其余为冷数据。对于冷数据不会再写入新数据，可以考虑定期force_merge加shrink压缩操作，节省存储空间和检索效率



### 部署层面

一旦之前没有规划，这里就属于应急策略。

结合 ES 自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力，注意：如果之前主节点等规划合理，不需要重启集群也能完成动态新增的。
1）最好是64GB内存的物理机器，但实际上32GB和16GB机器用的比较多，但绝对不能少于8G，除非数据量特别少，这点需要和客户方面沟通并合理说服对方。
2）多个内核提供的额外并发远胜过稍微快一点点的时钟频率。
3）尽量使用SSD，因为查询和索引性能将会得到显著提升。
4）避免集群跨越大的地理距离，一般一个集群的所有节点位于一个数据中心中。
5）设置堆内存：节点内存/2，不要超过32GB。一般来说设置export ES_HEAP_SIZE=32g环境变量，比直接写-Xmx32g -Xms32g更好一点。
6）关闭缓存swap。内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个100微秒的操作可能变成10毫秒。 再想想那么多10微秒的操作时延累加起来。不难看出swapping对于性能是多么可怕。
7）增加文件描述符，设置一个很大的值，如65535。Lucene使用了大量的文件，同时，Elasticsearch在节点和HTTP客户端之间进行通信也使用了大量的套接字。所有这一切都需要足够的文件描述符。
8）不要随意修改垃圾回收器（CMS）和各个线程池的大小。
9）通过设置gateway.recover_after_nodes、gateway.expected_nodes、gateway.recover_after_time可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。





## Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法

面试官：想了解对 ES 集群的运维能力。

解答：

1. 关闭缓存 swap;
2. 堆内存设置为：Min（节点内存/2, 32GB）;
3. 设置最大文件句柄数；
4. 线程池+队列大小根据业务需要做调整；
5. 磁盘存储 raid 方式——存储有条件使用 RAID10，增加单节点性能以及避免单节点存储故障。



## 在并发情况下，Elasticsearch如果保证读写一致？

可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；
另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。
对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。





## 介绍下你们电商搜索的整体技术架构。





## 拼写纠错是如何实现的？





## 介绍一下你们的个性化搜索方案？





## 如何监控Elasticsearch集群状态？



## translog日志文件作用是什么？










[toc]



# Java 并发简介 

## 1. 并发概念

并发编程中有很多术语概念相近，容易让人混淆。本节内容通过对比分析，力求让读者清晰理解其概念以及差异。

### 1.1. 并发和并行

并发和并行是最容易让新手费解的概念，那么如何理解二者呢？其最关键的差异在于：是否是**同时**发生：

- **并发**：是指具备处理多个任务的能力，但不一定要同时。
- **并行**：是指具备同时处理多个任务的能力。

下面是我见过最生动的说明，摘自 [并发与并行的区别是什么？——知乎的高票答案](https://www.zhihu.com/question/33515481/answer/58849148)：

- 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。
- 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。
- 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。

### 1.2. 同步和异步

- **同步**：是指在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。
- **异步**：则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。

举例来说明：

- 同步就像是打电话：不挂电话，通话不会结束。
- 异步就像是发短信：发完短信后，就可以做其他事；当收到回复短信时，手机会通过铃声或振动来提醒。

### 1.3. 阻塞和非阻塞

阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态：

- **阻塞**：是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。
- **非阻塞**：是指在不能立刻得到结果之前，该调用不会阻塞当前线程。

举例来说明：

- 阻塞调用就像是打电话，通话不结束，不能放下。
- 非阻塞调用就像是发短信，发完短信后，就可以做其他事，短信来了，手机会提醒。

### 1.4. 进程和线程

- **进程**：进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动。进程是操作系统进行资源分配的基本单位。进程可视为一个正在运行的程序。
- **线程**：线程是操作系统进行调度的基本单位。

进程和线程的差异：

- 一个程序至少有一个进程，一个进程至少有一个线程。
- 线程比进程划分更细，所以执行开销更小，并发性更高
- 进程是一个实体，拥有独立的资源；而同一个进程中的多个线程共享进程的资源。

![687474703a2f2f64756e77752e746573742e757063646e2e6e65742f63732f6a6176612f6a617661636f72652f636f6e63757272656e742f70726f6365737365732d76732d746872656164732e6a7067](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204652.jpg)

JVM 在单个进程中运行，JVM 中的线程共享属于该进程的堆。这就是为什么几个线程可以访问同一个对象。线程共享堆并拥有自己的堆栈空间。这是一个线程如何调用一个方法以及它的局部变量是如何保持线程安全的。但是堆不是线程安全的并且为了线程安全必须进行同步。

### 1.5. 竞态条件和临界区

- **竞态条件（Race Condition）**：当两个线程竞争同一资源时，如果对资源的访问顺序敏感，就称存在竞态条件。
- **临界区（Critical Sections）**：导致竞态条件发生的代码区称作临界区。

### 1.6. 管程

管程（Monitor），是指管理共享变量以及对共享变量的操作过程，让他们支持并发。

Java 采用的是管程技术，synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分。而**管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程**。

## 2. 并发的特点

技术在进步，CPU、内存、I/O 设备的性能也在不断提高。但是，始终存在一个核心矛盾：**CPU、内存、I/O 设备存在速度差异**。CPU 远快于内存，内存远快于 I/O 设备。

木桶短板理论告诉我们：一只木桶能装多少水，取决于最短的那块木板。同理，程序整体性能取决于最慢的操作（即 I/O 操作），所以单方面提高 CPU、内存的性能是无效的。

![687474703a2f2f64756e77752e746573742e757063646e2e6e65742f736e61702f32303230313232353137303035322e6a7067](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204703.jpg)

为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为：

- **CPU 增加了缓存**，以均衡与内存的速度差异；
- **操作系统增加了进程、线程**，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异；
- **编译程序优化指令执行次序**，使得缓存能够得到更加合理地利用。

其中，进程、线程使得计算机、程序有了并发处理任务的能力。

并发的优点在于：

- 提升资源利用率
- 程序响应更快

### 2.1. 提升资源利用率

想象一下，一个应用程序需要从本地文件系统中读取和处理文件的情景。比方说，从磁盘读取一个文件需要 5 秒，处理一个文件需要 2 秒。处理两个文件则需要：

```
5秒读取文件A
2秒处理文件A
5秒读取文件B
2秒处理文件B
---------------------
总共需要14秒
```

从磁盘中读取文件的时候，大部分的 CPU 时间用于等待磁盘去读取数据。在这段时间里，CPU 非常的空闲。它可以做一些别的事情。通过改变操作的顺序，就能够更好的使用 CPU 资源。看下面的顺序：

```
5秒读取文件A
5秒读取文件B + 2秒处理文件A
2秒处理文件B
---------------------
总共需要12秒
```

CPU 等待第一个文件被读取完。然后开始读取第二个文件。当第二文件在被读取的时候，CPU 会去处理第一个文件。记住，在等待磁盘读取文件的时候，CPU 大 部分时间是空闲的。

总的说来，CPU 能够在等待 IO 的时候做一些其他的事情。这个不一定就是磁盘 IO。它也可以是网络的 IO，或者用户输入。通常情况下，网络和磁盘的 IO 比 CPU 和内存的 IO 慢的多。

### 2.2. 程序响应更快

将一个单线程应用程序变成多线程应用程序的另一个常见的目的是实现一个响应更快的应用程序。设想一个服务器应用，它在某一个端口监听进来的请求。当一个请求到来时，它去处理这个请求，然后再返回去监听。

服务器的流程如下所述：

```
while(server is active) {
    listen for request
    process request
}
```

如果一个请求需要占用大量的时间来处理，在这段时间内新的客户端就无法发送请求给服务端。只有服务器在监听的时候，请求才能被接收。另一种设计是，监听线程把请求传递给工作者线程(worker thread)，然后立刻返回去监听。而工作者线程则能够处理这个请求并发送一个回复给客户端。这种设计如下所述：

```
while(server is active) {
    listen for request
    hand request to worker thread
}
```

这种方式，服务端线程迅速地返回去监听。因此，更多的客户端能够发送请求给服务端。这个服务也变得响应更快。

桌面应用也是同样如此。如果你点击一个按钮开始运行一个耗时的任务，这个线程既要执行任务又要更新窗口和按钮，那么在任务执行的过程中，这个应用程序看起来好像没有反应一样。相反，任务可以传递给工作者线程（worker thread)。当工作者线程在繁忙地处理任务的时候，窗口线程可以自由地响应其他用户的请求。当工作者线程完成任务的时候，它发送信号给窗口线程。窗口线程便可以更新应用程序窗口，并显示任务的结果。对用户而言，这种具有工作者线程设计的程序显得响应速度更快。

### 2.3. 并发的问题

任何事物都有利弊，并发也不例外。

我们知道了并发带来的好处：提升资源利用率、程序响应更快，同时也要认识到并发带来的问题，主要有：

- 安全性问题
- 活跃性问题
- 性能问题

下面会一一讲解。

## 3. 安全性问题

并发最重要的问题是并发安全问题。

**并发安全**：是指保证程序的正确性，使得并发处理结果符合预期。

并发安全需要保证几个基本特性：

- **可见性** - 是一个线程修改了某个共享变量，其状态能够立即被其他线程知晓，通常被解释为将线程本地状态反映到主内存上，`volatile` 就是负责保证可见性的。
- **原子性** - 简单说就是相关操作不会中途被其他线程干扰，一般通过同步机制（加锁：`sychronized`、`Lock`）实现。
- **有序性** - 是保证线程内串行语义，避免指令重排等。

### 3.1. 缓存导致的可见性问题

> 一个线程对共享变量的修改，另外一个线程能够立刻看到，称为 **可见性**。

在单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据一致性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的。例如在下面的图中，线程 A 和线程 B 都是操作同一个 CPU 里面的缓存，所以线程 A 更新了变量 V 的值，那么线程 B 之后再访问变量 V，得到的一定是 V 的最新值（线程 A 写过的值）。

![687474703a2f2f64756e77752e746573742e757063646e2e6e65742f736e61702f32303230303730313131303331332e706e67](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204717.png)

多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存。比如下图中，线程 A 操作的是 CPU-1 上的缓存，而线程 B 操作的是 CPU-2 上的缓存，很明显，这个时候线程 A 对变量 V 的操作对于线程 B 而言就不具备可见性了。

![687474703a2f2f64756e77752e746573742e757063646e2e6e65742f736e61702f32303230303730313131303433312e706e67](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204735.png)

【示例】线程不安全的示例

下面我们再用一段代码来验证一下多核场景下的可见性问题。下面的代码，每执行一次 add10K() 方法，都会循环 10000 次 count+=1 操作。在 calc() 方法中我们创建了两个线程，每个线程调用一次 add10K() 方法，我们来想一想执行 calc() 方法得到的结果应该是多少呢？

```
public class Test {
  private long count = 0;
  private void add10K() {
    int idx = 0;
    while(idx++ < 10000) {
      count += 1;
    }
  }
  public static long calc() {
    final Test test = new Test();
    // 创建两个线程，执行 add() 操作
    Thread th1 = new Thread(()->{
      test.add10K();
    });
    Thread th2 = new Thread(()->{
      test.add10K();
    });
    // 启动两个线程
    th1.start();
    th2.start();
    // 等待两个线程执行结束
    th1.join();
    th2.join();
    return count;
  }
}
```

直觉告诉我们应该是 20000，因为在单线程里调用两次 add10K() 方法，count 的值就是 20000，但实际上 calc() 的执行结果是个 10000 到 20000 之间的随机数。为什么呢？

我们假设线程 A 和线程 B 同时开始执行，那么第一次都会将 count=0 读到各自的 CPU 缓存里，执行完 count+=1 之后，各自 CPU 缓存里的值都是 1，同时写入内存后，我们会发现内存中是 1，而不是我们期望的 2。之后由于各自的 CPU 缓存里都有了 count 的值，两个线程都是基于 CPU 缓存里的 count 值来计算，所以导致最终 count 的值都是小于 20000 的。这就是缓存的可见性问题。

循环 10000 次 count+=1 操作如果改为循环 1 亿次，你会发现效果更明显，最终 count 的值接近 1 亿，而不是 2 亿。如果循环 10000 次，count 的值接近 20000，原因是两个线程不是同时启动的，有一个时差。

![687474703a2f2f64756e77752e746573742e757063646e2e6e65742f736e61702f32303230303730313131303631352e706e67](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204751.png)

### 3.2. 线程切换带来的原子性问题

由于 IO 太慢，早期的操作系统就发明了多进程，操作系统允许某个进程执行一小段时间（称为 **时间片**）。

在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。

这里的进程在等待 IO 时之所以会释放 CPU 使用权，是为了让 CPU 在这段等待时间里可以做别的事情，这样一来 CPU 的使用率就上来了；此外，如果这时有另外一个进程也读文件，读文件的操作就会排队，磁盘驱动在完成一个进程的读操作后，发现有排队的任务，就会立即启动下一个读操作，这样 IO 的使用率也上来了。

早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。

Java 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，例如上面代码中的 `count += 1`，至少需要三条 CPU 指令。

- 指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器；
- 指令 2：之后，在寄存器中执行 +1 操作；
- 指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存）。

操作系统做任务切换，可以发生在任何一条**CPU 指令**执行完，是的，是 CPU 指令，而不是高级语言里的一条语句。对于上面的三条指令来说，我们假设 count=0，如果线程 A 在指令 1 执行完后做线程切换，线程 A 和线程 B 按照下图的序列执行，那么我们会发现两个线程都执行了 count+=1 的操作，但是得到的结果不是我们期望的 2，而是 1。

![687474703a2f2f64756e77752e746573742e757063646e2e6e65742f736e61702f32303230303730313131303934362e706e67](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204804.png)

我们潜意识里面觉得 count+=1 这个操作是一个不可分割的整体，就像一个原子一样，线程的切换可以发生在 count+=1 之前，也可以发生在 count+=1 之后，但就是不会发生在中间。**我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性**。CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方。因此，很多时候我们需要在高级语言层面保证操作的原子性。

### 3.3. 编译优化带来的有序性问题

那并发编程里还有没有其他有违直觉容易导致诡异 Bug 的技术呢？有的，就是有序性。顾名思义，有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果。不过有时候编译器及解释器的优化可能导致意想不到的 Bug。

在 Java 领域一个经典的案例就是利用双重检查创建单例对象，例如下面的代码：在获取实例 getInstance() 的方法中，我们首先判断 instance 是否为空，如果为空，则锁定 Singleton.class 并再次检查 instance 是否为空，如果还为空则创建 Singleton 的一个实例。

```
public class Singleton {
  static Singleton instance;
  static Singleton getInstance(){
    if (instance == null) {
      synchronized(Singleton.class) {
        if (instance == null)
          instance = new Singleton();
        }
    }
    return instance;
  }
}
```

假设有两个线程 A、B 同时调用 getInstance() 方法，他们会同时发现 `instance == null` ，于是同时对 Singleton.class 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）；线程 A 会创建一个 Singleton 实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 `instance == null` 时会发现，已经创建过 Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。

这看上去一切都很完美，无懈可击，但实际上这个 getInstance() 方法并不完美。问题出在哪里呢？出在 new 操作上，我们以为的 new 操作应该是：

1. 分配一块内存 M；
2. 在内存 M 上初始化 Singleton 对象；
3. 然后 M 的地址赋值给 instance 变量。

但是实际上优化后的执行路径却是这样的：

1. 分配一块内存 M；
2. 将 M 的地址赋值给 instance 变量；
3. 最后在内存 M 上初始化 Singleton 对象。

优化后会导致什么问题呢？我们假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 `instance != null` ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。

![](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204826.png)

### 3.4. 保证并发安全的思路

#### 互斥同步（阻塞同步）

互斥同步是最常见的并发正确性保障手段。

**同步是指在多线程并发访问共享数据时，保证共享数据在同一时刻只能被一个线程访问**。

互斥是实现同步的一种手段。临界区（Critical Sections）、互斥量（Mutex）和信号量（Semaphore）都是主要的互斥实现方式。

最典型的案例是使用 `synchronized` 或 `Lock` 。

**互斥同步最主要的问题是线程阻塞和唤醒所带来的性能问题**，互斥同步属于一种悲观的并发策略，总是认为只要不去做正确的同步措施，那就肯定会出现问题。无论共享数据是否真的会出现竞争，它都要进行加锁（这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁）、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作。

#### 非阻塞同步

随着硬件指令集的发展，我们可以使用基于冲突检测的乐观并发策略：先进行操作，如果没有其它线程争用共享数据，那操作就成功了，否则采取补偿措施（不断地重试，直到成功为止）。这种乐观的并发策略的许多实现都不需要将线程阻塞，因此这种同步操作称为非阻塞同步。

为什么说乐观锁需要 **硬件指令集的发展** 才能进行？因为需要操作和冲突检测这两个步骤具备原子性。而这点是由硬件来完成，如果再使用互斥同步来保证就失去意义了。

这类乐观锁指令常见的有：

- 测试并设置（Test-amd-Set）
- 获取并增加（Fetch-and-Increment）
- 交换（Swap）
- 比较并交换（CAS）
- 加载链接、条件存储（Load-linked / Store-Conditional）

Java 典型应用场景：J.U.C 包中的原子类（基于 `Unsafe` 类的 CAS 操作）

#### 无同步

要保证线程安全，不一定非要进行同步。同步只是保证共享数据争用时的正确性，如果一个方法本来就不涉及共享数据，那么自然无须同步。

Java 中的 **无同步方案** 有：

- **可重入代码** - 也叫纯代码。如果一个方法，它的 **返回结果是可以预测的**，即只要输入了相同的数据，就能返回相同的结果，那它就满足可重入性，当然也是线程安全的。
- **线程本地存储** - 使用 **`ThreadLocal` 为共享变量在每个线程中都创建了一个本地副本**，这个副本只能被当前线程访问，其他线程无法访问，那么自然是线程安全的。

## 4. 活跃性问题

### 4.1. 死锁（Deadlock）

#### 什么是死锁

多个线程互相等待对方释放锁。

死锁是当线程进入无限期等待状态时发生的情况，因为所请求的锁被另一个线程持有，而另一个线程又等待第一个线程持有的另一个锁。

![687474703a2f2f64756e77752e746573742e757063646e2e6e65742f63732f6a6176612f6a617661636f72652f636f6e63757272656e742f646561646c6f636b2e706e67](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204906.png)

#### 避免死锁

（1）按序加锁

当多个线程需要相同的一些锁，但是按照不同的顺序加锁，死锁就很容易发生。

如果能确保所有的线程都是按照相同的顺序获得锁，那么死锁就不会发生。

按照顺序加锁是一种有效的死锁预防机制。但是，这种方式需要你事先知道所有可能会用到的锁(译者注：并对这些锁做适当的排序)，但总有些时候是无法预知的。

（2）超时释放锁

另外一个可以避免死锁的方法是在尝试获取锁的时候加一个超时时间，这也就意味着在尝试获取锁的过程中若超过了这个时限该线程则放弃对该锁请求。若一个线程没有在给定的时限内成功获得所有需要的锁，则会进行回退并释放所有已经获得的锁，然后等待一段随机的时间再重试。这段随机的等待时间让其它线程有机会尝试获取相同的这些锁，并且让该应用在没有获得锁的时候可以继续运行(译者注：加锁超时后可以先继续运行干点其它事情，再回头来重复之前加锁的逻辑)。

（3）死锁检测

死锁检测是一个更好的死锁预防机制，它主要是针对那些不可能实现按序加锁并且锁超时也不可行的场景。

每当一个线程获得了锁，会在线程和锁相关的数据结构中（map、graph 等等）将其记下。除此之外，每当有线程请求锁，也需要记录在这个数据结构中。

当一个线程请求锁失败时，这个线程可以遍历锁的关系图看看是否有死锁发生。

如果检测出死锁，有两种处理手段：

- 释放所有锁，回退，并且等待一段随机的时间后重试。这个和简单的加锁超时类似，不一样的是只有死锁已经发生了才回退，而不会是因为加锁的请求超时了。虽然有回退和等待，但是如果有大量的线程竞争同一批锁，它们还是会重复地死锁（编者注：原因同超时类似，不能从根本上减轻竞争）。
- 一个更好的方案是给这些线程设置优先级，让一个（或几个）线程回退，剩下的线程就像没发生死锁一样继续保持着它们需要的锁。如果赋予这些线程的优先级是固定不变的，同一批线程总是会拥有更高的优先级。为避免这个问题，可以在死锁发生的时候设置随机的优先级。

### 4.2. 活锁（Livelock）

#### 什么是活锁

活锁是一个递归的情况，两个或更多的线程会不断重复一个特定的代码逻辑。预期的逻辑通常为其他线程提供机会继续支持'this'线程。

想象这样一个例子：两个人在狭窄的走廊里相遇，二者都很礼貌，试图移到旁边让对方先通过。但是他们最终在没有取得任何进展的情况下左右摇摆，因为他们都在同一时间向相同的方向移动。

![687474703a2f2f64756e77752e746573742e757063646e2e6e65742f63732f6a6176612f6a617661636f72652f636f6e63757272656e742f6c6976656c6f636b2e706e67](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204925.png)

如图所示：两个线程想要通过一个 Worker 对象访问共享公共资源的情况，但是当他们看到另一个 Worker（在另一个线程上调用）也是“活动的”时，它们会尝试将该资源交给其他工作者并等待为它完成。如果最初我们让两名工作人员都活跃起来，他们将会面临活锁问题。

#### 避免活锁

解决“**活锁**”的方案很简单，谦让时，尝试等待一个随机的时间就可以了。由于等待的时间是随机的，所以同时相撞后再次相撞的概率就很低了。“等待一个随机时间”的方案虽然很简单，却非常有效，Raft 这样知名的分布式一致性算法中也用到了它。

### 4.3. 饥饿（Starvation）

#### 什么是饥饿

- 高优先级线程吞噬所有的低优先级线程的 CPU 时间。
- 线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访问。
- 线程在等待一个本身(在其上调用 wait())也处于永久等待完成的对象，因为其他线程总是被持续地获得唤醒。

![687474703a2f2f64756e77752e746573742e757063646e2e6e65742f63732f6a6176612f6a617661636f72652f636f6e63757272656e742f73746172766174696f6e2d616e642d666169726e6573732e706e67](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210322204935.png)

饥饿问题最经典的例子就是哲学家问题。如图所示：有五个哲学家用餐，每个人要活得两把叉子才可以就餐。当 2、4 就餐时，1、3、5 永远无法就餐，只能看着盘中的美食饥饿的等待着。

#### 解决饥饿

Java 不可能实现 100% 的公平性，我们依然可以通过同步结构在线程间实现公平性的提高。

有三种方案：

- 保证资源充足
- 公平地分配资源
- 避免持有锁的线程长时间执行

这三个方案中，方案一和方案三的适用场景比较有限，因为很多场景下，资源的稀缺性是没办法解决的，持有锁的线程执行的时间也很难缩短。倒是方案二的适用场景相对来说更多一些。

那如何公平地分配资源呢？在并发编程里，主要是使用**公平锁**。所谓公平锁，是一种先来后到的方案，线程的等待是有顺序的，排在等待队列前面的线程会优先获得资源。

## 5. 性能问题

并发执行一定比串行执行快吗？线程越多执行越快吗？

答案是：**并发不一定比串行快**。因为有创建线程和线程上下文切换的开销。

### 5.1. 上下文切换

#### 什么是上下文切换？

当 CPU 从执行一个线程切换到执行另一个线程时，CPU 需要保存当前线程的本地数据，程序指针等状态，并加载下一个要执行的线程的本地数据，程序指针等。这个开关被称为“上下文切换”。

#### 减少上下文切换的方法

- 无锁并发编程 - 多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的 ID 按照 Hash 算法取模分段，不同的线程处理不同段的数据。
- CAS 算法 - Java 的 Atomic 包使用 CAS 算法来更新数据，而不需要加锁。
- 使用最少线程 - 避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态。
- 使用协程 - 在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。

### 5.2. 资源限制

#### 什么是资源限制

资源限制是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。

#### 资源限制引发的问题

在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变成并发执行，但是如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的时间。

#### 如何解决资源限制的问题

在资源限制情况下进行并发编程，根据不同的资源限制调整程序的并发度。

- 对于硬件资源限制，可以考虑使用集群并行执行程序。
- 对于软件资源限制，可以考虑使用资源池将资源复用。

### 5.3.Java多线程引发的性能问题以及调优策略

#### 无限制创建线程

Web服务器中，在正常负载情况下，为每个任务分配一个线程，能够提升串行执行条件下的性能。只要请求的到达率不超出服务器的请求处理能力，那么这种方法可以同时带来更快的响应性和更高的吞吐率。如果请求的到达速率非常高，且请求的处理过程是轻量级的，那么为每个请求创建一个新线程将消耗大量的计算资源。

**引发的问题**

- 线程的生命周期开销非常高

- 消耗过多的CPU资源

  如果可运行的线程数量多于可用处理器的数量，那么有线程将会被闲置。大量空闲的线程会占用许多内存，给垃圾回收器带来压力，而且大量的线程在竞争CPU资源时还将产生其他性能的开销。

- 降低稳定性

  JVM在可创建线程的数量上存在一个限制，这个限制值将随着平台的不同而不同，并且承受着多个因素制约，包括JVM的启动参数、Thread构造函数中请求栈的大小，以及底层操作系统对线程的限制等。如果破坏了这些限制，那么可能抛出OutOfMemoryError异常。

**调优策略**

可以使用线程池，是指管理一组同构工作线程的资源池。

线程池的本质就是：有一个队列，任务会被提交到这个队列中。一定数量的线程会从该队列中取出任务，然后执行。任务的结果可以发回客户端、可以写入数据库、也可以存储到内部数据结构中，等等。但是任务执行完成后，这个线程会返回任务队列，检索另一个任务并执行。

使用线程池可以带来以下的好处：

- 通过重用现有的线程而不是创建新线程，可以在处理多个请求时分摊在线程创建和销毁过程中产生的巨大开销。
- 当请求到达时，工作线程已经存在，因此不会由于等待创建线程而延迟任务的执行，从而提高了响应性。
- 通过适当调整线程池大小，可以创建足够多的线程以便使处理器保持忙碌状态，同时还可以防止过多线程相互竞争资源而使应用程序耗尽内存或失败。

#### 线程同步

**引发的问题**

- 降低可伸缩性

  在有些问题中，如果可用资源越多，那么问题的解决速度就越快。如果使用多线程主要是为了发挥多个处理器的处理能力，那么就必须对问题进行合理的并行分解，并使得程序能够有效地使用这种潜在的并行能力。

  不过大多数的并发程序都是由一系列的并行工作和串行工作组成的。因此Amdhl定律描述的是：在增加计算资源的情况下，程序在理论上能够实现最高加速度比，这个值取决于程序中可并行组件(1-F)与串行组件(F)所占的比重。 

  ![image-20210324235229653](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210324235229.png)

  - 当N趋近于无穷大时，最大的加速度比趋近于1/F1/F 

    - 如果程序有50%的计算资源需要串行执行，那么最高的加速度比是能是2（而不管有多少个线程可用）。

    - 如果在程序中有10%的计算需要串行执行，那么最高的加速度比将接近10。

  - 如果程序中有10%的部分需要串行执行 
    - 在拥有10个处理器的系统中，那么最高的加速度比为5.3（53%的使用率）；
    - 在拥有100个处理器的系统中，加速度比可以达到9.2（9%的使用率）；

  因此，随着F值的增大（也就是说有更多的代码是串行执行的），那么引入多线程带来的优势也随之降低。所以也说明了限制串行块的代码量非常重要。

- 上下文切换开销

  如果主线程是唯一的线程，那么它基本上不会被调度出去。如果可运行的线程数大于CPU的数量，那么操作系统最终会将某个正在运行的线程调度出来，从而使其他线程能够使用CPU。这将导致一次上下文切换，这个过程将保存当前运行线程的执行上下文，并将新调度进来的线程的执行上下文设置为当前上下文。

  那么在上下文切换的时候将导致以下的开销

  - 在线程调度过程中需要访问由操作系统和JVM共享的数据结构。
  - 应用程序、操作系统以及JVM都使用一组相同的CPU，在JVM和操作系统的代码中消耗越多的CPU时钟周期，应用程序的可用CPU时钟周期就越来越少。
  - 当一个新的线程被切换进来时，它所需要的数据可能不在当前处理器的本地缓存中，因此上下文切换将导致一些缓存缺失，因而线程在首次调度运行时会更加缓慢。

  这就是为什么调度器会为每个可运行的线程分配一个最小执行时间，即使有许多其他的线程正在执行——它将上下文切换的开销分摊到更多不会中断的执行时间上，从而提高整体的吞吐量（以损失响应性为代价）。

  当线程由于等待某个发生竞争的锁而被阻塞时，JVM通常会将这个线程挂起，并允许它被交换出去。如果线程频繁地发生阻塞，那么它将无法获得完整的调度时间片。在程序中发生越来越多的阻塞，与CPU密集型的程序就会发生越多的上下文切换，从而增加调度开销，并因此降低吞吐量（无阻塞算法同样有助于减少上下文切换）。

- 内存同步开销
  - 内存栅栏间接带来的影响

    在synchronized和volatile提供的可见性保证中可能会使用一些特殊指令，即内存栅栏（Memory Barrier），内存栅栏可以刷新缓存，使缓存无效，刷新硬件的写缓冲，以及停止执行管道。

    内存栅栏可能同样会对性能带来间接的影响，因为他们将抑制一些编译器优化操作。并且在内存栅栏中，大多数操作都是不能被重排序的。

  - 竞争产生的同步可能需要操作系统的介入，从而增加开销

    在锁上发生竞争的时候，竞争失败的线程肯定会阻塞。JVM在实现阻塞行为时，可以采用自旋等待（Spin-Waiting，指通过循环不断地尝试获取锁，直到成功），或者通过操作系统挂起被阻塞的线程。这两种方式的效率高低，取决于上下文切换的开销以及在成功获取锁之前需要等待的时间。如果等待时间较短，则适合采用自旋等待的方式，而如果等待时间较长，则适合采用线程挂起方式。

    某个线程中的同步可能会影响其他线程的性能，同步会增加内存总线上的通信量，总线的带宽是有限的，并且所有的处理器都将共享这条总线。如果有多个线程竞争同步带宽，那么所有使用同步的线程都会受到影响。

  - 无竞争的同步带来的开销可忽略

    synchronized机制针对无竞争的同步进行了优化，去掉一些不会发生竞争的锁，从而减少不必要的同步开销。所以，不要担心非竞争同步带来的开销，这个基本的机制已经非常快了，并且JVM还能进行额外的优化以进一步降低或消除开销。

    - 如果一个对象只能由当前线程访问，那么JVM就可以通过优化来去掉这个锁获取操作。

    - 一些完备的JVM能通过逸出分析来找出不会发布到堆的本地对象引用（这些引用是线程本地的）

      在getStoogeNames()的执行过程中，至少会将Vector上的锁获取释放4次，每次调用add或toString时都会执行一次。然而，一个智能的运行时编译器通常会分析这些调用，从而使stooges及其内部状态不会逸出，因此可以去掉这4次对锁的获取操作。

        ```
        public String getStoogeNames(){
         List<String> stooges = new Vector<>();
         stooges.add("Moe");
         stooges.add("Larry");
         stooges.add("Curly");
         return stooges.toString();
        }
        ```

		- 即使不进行逸出分析，编译器也可以执行锁粒度粗化操作，将临近的同步代码块用同一个锁合并起来。在getStoogeNames中，如果JVM进行锁粒度粗化，那么可能会把3个add和1个toString调用合并为单个锁获取/释放操作，并采用启发式方法来评估同步代码块中采用同步操作以及指令之间的相对开销。这不仅减少了同步的开销，同时还能使优化处理更大的代码块，从而可能实现进一步的优化。

**调优策略**

- 避免同步
  - 使用线程局部变量ThreadLocal

    ThreadLocal类能够使线程的某个值与保存该值的线程对象关联起来。ThreadLocal提供了get和set等方法，这些方法使每个使用该变量的线程都存有一个独立的副本，因此get总是返回由当前执行线程在调用set设置的最新值。

    当某个线程初次调用ThreadLocal.get方法时，就会调用initialValue来获取初始值。这些特定于线程的值保存在Thread对象中，当线程终止后，这些值会作为垃圾回收。

    ```
    private static ThreadLocal<Connection> connectionHolder = 
           ThreadLocal.withInitial(() -> DriverManager.getConnecton(DB_URL));

    public static Connection getConnection(){
       return connectionHolder.get();
    }
    ```

	- 使用基于CAS的替代方案
	
	  在某种意义上，这不是避免同步，而是减少同步带来的性能损失。通常情况下，在基于比较的CAS和传统的同步时，有以下使用原则：
	
	  - 如果访问的是不存在竞争的资源，那么基于CAS的保护稍快于传统同步（完全不保护会更快）；
	
	  - 如果访问的资源存在轻度或适度的竞争，那么基于CAS的保护要快于传统同步（往往是块的多）；
	
	  - 如果访问的资源竞争特别激烈，这时，传统的同步是更好的选择。
	
	    对于该结论可以这么理解，在其他领域依然成立：当交通拥堵时，交通信号灯能够实现更高的吞吐量，而在低拥堵时，环岛能实现更高的吞吐量。这是因为锁在发生竞争时会挂起线程，从而降低了CPU的使用率和共享内存总线上的同步通信量。类似于在生产者-消费者模式中，可阻塞生产者，它能降低消费者上的工作负载，使消费者的处理速度赶上生产者的处理速度。
	
- 减少锁竞争

  串行操作会降低可伸缩性，在并发程序中，对可伸缩性的最主要威胁就是独占方式的资源锁。在锁上竞争时，将同时导致可伸缩性和上下文切换问题，因此减少锁的竞争能够提高性能和可伸缩性。

  在锁上发生竞争的可能性主要由两个因素影响：锁的请求频率和每次持有该锁的时间。

  如果两者的乘积很小，那么大多数获取锁的操作都不会发生竞争，因此在该锁上的竞争不会对可伸缩性造成影响。

  如果在锁上的请求量非常高，那么需要获取该锁的线程将被阻塞并等待。

  因此，有3种方式可以降低锁的竞争程度：
  - 减少锁的持有时间——主要通过缩小锁的范围，快进快出

  	- 将一个与锁无关的操作移除同步代码块，尤其是那些开销较大的操作，以及可能被阻塞的操作。

  	- 通过将线程安全性委托给其他线程安全类来进一步提升它的性能。这样就无需使用显式的同步，缩小了锁范围，并降低了将来代码维护无意破坏线程安全性的风险。
  	
  	- 尽管缩小同步代码块能提高可伸缩性，但同步代码块也不能过小——一些需要采用原子方式执行的操作必须包含在同一个块中。同步还需要一定的开销，把一个同步代码块分解为多个同步代码块时，反而会对性能产生负面影响。
  	
  - 降低锁的请求频率
  
    通过锁分解和锁分段等技术来实现，将采用多个相互独立的锁来保护独立的状态变量，从而改变这些变量在之前由单个锁来保护的情况。也就是说，如果一个锁需要保护多个相互独立的状态变量，那么可以将这个锁分解为多个锁，并且每个锁只保护一个变量，从而提高可伸缩性，并最终降低每个锁被请求的频率。然而，使用的锁越多，那么发生死锁的风险也就越高。
  
    - 如果在锁上存在适中而不是激烈的竞争，通过将一个锁分解为两个锁，能最大限度地提升性能。如果对竞争并不激烈的锁进行分解，那么在性能和吞吐量等方面带来的提升将非常有限，但是也会提高性能随着竞争而下降的拐点值。对竞争适中的锁进行分解时，实际上是把这些锁转变为非竞争的锁，从而有效地提高性能和可伸缩性。

        ```
        public class ServerStatus {
         private Set<String> users;
         private Set<String> queries;

         public synchronized void addUser(String u) {
             users.add(u);
         }

         public synchronized void addQuery(String u) {
             queries.add(u);
         }

         public synchronized void removeUser(String u) {
             users.remove(u);
         }

         public synchronized void removeQuery(String q) {
             queries.remove(q);
         }
        }
        // 使用锁分解技术
        public class ServerStatus {
         private Set<String> users;
         private Set<String> queries;

         public void addUser(String u) {
             synchronized (users) {
                 users.add(u);
             }
         }

         public void addQuery(String u) {
             synchronized (queries) {
                 queries.add(u);
             }
         }

         public void removeUser(String u) {
             synchronized (users) {
                 users.remove(u);
             }
         }

         public void removeQuery(String q) {
             synchronized (queries) {
                 queries.remove(q);
             }
         }
        }
        ```
        
    - 在某些情况下，可以将锁分解技术进一步扩展为对一组独立对象上的锁进行分解，这种情况被称为锁分段。
    
        在ConcurrentHashMap的实现中，使用了一个包含16个锁的数组，每个锁保护所有散列桶的1/16，其中第N个散列通有第(N mod 16N mod 16)个锁来保护。假设散列函数具有合理的分布性，并且关键字能够实现均匀分布，那么大约能把对于锁的请求减少到原来的1/16。正是这项技术使得ConcurrentHashMap能够支持多达16个并发的写入器。

        ```
            public class StripedMap {
             private static final int N_LOCKS = 16;
             private final Node[] buckets;
             private final Object[] locks;

             static class Node<K, V> {
                 final int hash;
                 final K key;
                 V value;
                 Node<K, V> next;

                 public Node(int hash, K key) {
                     this.hash = hash;
                     this.key = key;
                 }
             }

             public StripedMap(int capacity) {
                 this.buckets = new Node[capacity];
                 this.locks = new Object[N_LOCKS];
                 for (int i = 0; i < N_LOCKS; i++) {
                     locks[i] = new Object();
                 }
             }

             private final int hash(Object key) {
                 return Math.abs(key.hashCode() % buckets.length);
             }

             public Object get(Object key) {
                 int hash = hash(key);
                 synchronized (locks[hash % N_LOCKS]) {
                     for (Node n = buckets[hash]; n != null; n = n.next) {
                         if (n.key.equals(key)) {
                             return n.value;
                         }
                     }
                 }
                 return null;
             }

             public void clear() {
                 for (int i = 0; i < buckets.length; i++) {
                     synchronized (locks[i % N_LOCKS]) {
                         buckets[i] = null;
                     }
                 }
             }
            }
        ```
        
        锁分段的一个劣势在于：与采用单个锁来实现独占访问相比，要获取多个锁来实现独占访问将更加困难并且开销更高。当ConcurrentHashMap需要扩展映射范围，以及重新计算键值的散列值要分布到更大的桶集合中时，就需要获取分段锁集合中的所有锁。
    
    锁分解和锁分段技术都能提高可伸缩性，因为他们都能使不同的线程在不同的数据（或者同一数据的不同部分）上操作，而不会相互干扰。如果程序使用锁分段技术，一定要表现在锁上的竞争频率高于在锁保护的数据上发生竞争的频率。
    
  - 避免热点区域
  
    在常见的优化措施中，就是将一个反复计算的结果缓存起来，都会引入一些热点区域，而这些热点区域往往会限制可伸缩性。在容器类中，为了获得容器的元素数量，使用了一个共享的计数器来统计size。在单线程或者采用完全同步的实现中，使用一个独立的计数器能很好地提高类似size和isEmpty这些方法的执行速度，但却导致更难以提升的可伸缩性，因此每个修改map的操作都要更新这个共享的计数器。即使使用锁分段技术来实现散列链，那么在对计数器的访问进行同步时，也会重新导致在使用独占锁时存在的可伸缩性问题。
  
    为了避免这个问题，ConcurrentHashMap中的size将对每个分段进行枚举，并将每个分段中的元素数量相加，而不是维护一个全局计数。为了避免枚举每个计数，ConcurrentHashMap为每个分段都维护了一个独立的计数，并通过每个分段的锁来维护这个值。
  
  - 放弃使用独占锁，使用一种友好并发的方式来管理共享状态
  
    - ReadWriteLock：实现了一种在多个读取操作以及单个写入操作情况下的加锁规则。
  
      如果多个读取操作都不会修改共享资源，那么这些读操作可以同时访问该共享资源，但是执行写入操作时必须以独占方式来获取锁。
  
      对于读取占多数的数据结构，ReadWriteLock能够提供比独占锁更高的并发性。而对于只读的数据结构，其中包含的不变形可以完全不需要加锁操作。
  
    - 原子变量：提供了一种方式来降低更新热点域时的开销。
  
      静态计数器、序列发生器、或者对链表数据结构中头结点的引用。如果在类中只包含了少量的共享状态，并且这些共享状态不会与其他变量参与到不变性条件中，那么用原子变量来替代他们能够提高可伸缩性。

- 使用偏向锁

  当锁被争用时，JVM可以选择如何分配锁。

	- 锁可以被公平地授予，每个线程以轮转调度方式获得锁；

	- 还有一种方案，即锁可以偏向于对它访问最为频繁的线程。

  偏向锁的理论依据是，如果一个线程最近用到了某个锁，那么线程下一次执行由同一把锁保护的代码所需的数据可能仍然保存在处理器的缓存中。如果给这个线程优先获得锁的权利，那么缓存命中率就会增加（支持老用户，避免新用户相关的开销）。那么性能就会有所改进，因为避免了新线程在当前处理器创建新的缓存的开销。
  
  但是，如果使用的编程模型是为了不同的线程池由同等机会争用锁，那么禁用偏向锁-XX:-UseBiasedLocking会改进性能。

- 使用自旋锁

  在处理同步锁竞争时，JVM有两种选择。
  - 可以让当前线程进入忙循环，执行一些指令，然后再次检查这个锁；
  - 也可以把这个线程放入一个队列挂起（使得CPU供其他线程可用），在锁可用时通知他。

  如果多个线程竞争的锁被持有时间短，那么自旋锁就是比较好的方案。如果锁被持有时间长，那么让第二个线程等待通知会更好。

  如果想影响JVM处理自旋锁的方式，唯一合理的方式就是让同步块尽可能的短。

#### 伪共享

**引发的问题**

在同步可能带来的影响方面，就是伪共享，它的出现跟CPU处理其高速缓存的方式有关。下面举一个极端的例子，有一个DataHolder的类：

```
public class DataHolder{
  public volatile long l1;
  public volatile long l2;
  public volatile long l3;
  public volatile long l4;
}
```

这里的每个long值都保存在毗邻的内存位置。例如，l1可能保存在0xF20位置，l2就会保存在0xF28位置，剩余的以此类推。当程序要操作l2时，会有一大块的内存（包括l2前后）被加载到当前所用的某个CPU核的缓存行（cache line）上。

大多数情况下，这么做是有意义的：如果程序访问了对象的某个特定实例，那么也可能访问邻接的实例变量。如果这些实例变量被加载到当前核的高速缓存中，那么内存访问就会特别快。

那么这种模式的缺点就是：当程序更新本地缓存中的某个值时，当前线程所在的核必须通知其他的所有核——这个内存被修改了。其他核必须作废其缓存行（cache line），并重新从内存中加载。那么随着线程数的增多，对volatile的操作越来越频繁，那么性能会逐渐降低。

Java内存模型要求数据只是在同步原语（包括CAS和volatile构造）结束时必须写入主内存。严格来讲，伪共享不一定会涉及同步（volatile）变量，如果long变量不是volatile，那么编译器会将这些值放到寄存器中，这样性能影响并没有那么大。然而不论何时，CPU缓存中有任何数据被写入，其他保存了同样范围数据的缓存都必须作废。

**调优策略**

很明显这是个极端的例子，但是提出了一个问题，如何检测并纠正伪共享？目前还不能解决伪共享，因为涉及处理器架构相关的专业知识，但是可以从代码入手：

- 避免所涉及的变量频繁的写入

  可以使用局部变量代替，只有最终结果才写回到volatile变量。随着写入次数的减少，对缓存行的竞争就会降低。

- 填充相关变量，避免其被加载到相同的缓存行中。

    ```
    public class DataHolder{
     public volatile long l1;
     public long[] dummy1 = new long[128/8];
     public volatile long l2;
     public long[] dummy2 = new long[128/8];
     public volatile long l3;
     public long[] dummy3 = new long[128/8];
     public volatile long l4; 
    }
    ```
    
    使用数组来填充变量或许行不通，因为JVM可能会重新安排实例变量的布局，以便使得所有数组挨在一起，于是所有的long变量就仍然紧挨着了。
    
    如果使用基本类型的值来填充该结构，行之有效的可能性大，但是对于变量的数目不好把控。
    
    另外，对于填充的大小也很难预测，因为不同的CPU缓存大小也不同，而且填充会增大实例，对垃圾收集影响很大。
    
    不过，如果没有算法上的改进方案，填充数据有时会具有明显的优势。

#### 线程池

**引发的问题**

- 线程饥饿死锁

  只要线程池中的任务需要无限期地等待一些必须由池中其他任务才能提供的资源或条件，例如某个任务等待另一个任务的返回值或执行结果，那么除非线程池足够大，否则将发生线程饥饿死锁。

  因此，每当提交了一个有依赖的Executor任务时，要清楚地知道可能会出现线程饥饿死锁，因此需要在代码或配置Executor的配置文件中记录线程池的大小或配置限制。

  如果任务阻塞的时间过长，那即使不出现死锁，线程池的响应性也会变得糟糕。执行时间过长的任务不仅会造成线程池阻塞，甚至还会增加执行时间较短任务的服务时间。

- 线程池过大对性能有不利的影响

  实现线程池有一个非常关键的因素：调节线程池的大小对获得最好的性能至关重要。线程池可以设置最大和最小线程数，池中会有最小线程数目的线程随时待命，如果任务量增长，可以往池中增加线程，最大线程数可以作为线程数的上限，防止运行太多线程反而造成性能的降低。

**调优策略**

- 设置最大线程数

  线程池的理想大小取决于被提交任务的类型以及所部署系统的特性。同时，设置线程池的大小需要避免“过大”和“过小”这两种极端情况。
  - 如果线程池过大，那么大量的线程将在相对很少的CPU和内存资源上发生竞争，这不仅会导致更高的内存使用量，而且还可能耗尽资源。
  - 如果线程池过小，那么将导致许多空闲的处理器无法执行工作，从而降低吞吐率。

  因此，要想正确地设置线程池的大小，必须分析计算环境、资源预算和任务的特性。在部署的系统中有都少个CPU？多大的内存？任务是计算密集型、I/O密集型还是二者皆可？他们是否需要像JDBC连接这样的稀缺资源？如果需要执行不同类别的任务，并且他们之间的行为相差很大，那么应该考虑使用多个线程池，从而使每个线程可以根据各自的工作负载来调整。

  要是处理器达到期望的使用率，线程池的最优大小等于：

  ![image-20210325000720614](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/java-core-demo/20210325000720.png)

  另外，CPU周期并不是唯一影响线程池大小的资源，还包括内存、文件句柄、套接字句柄和数据库连接等。通过计算每个任务对该资源的需求量，然后用该资源的可用总量除以每个任务的需求量，所得结果解释线程池大小的上限。

- 设置最小（核心）线程数

  可以将线程数设置为其他某个值，比如1。出发点是防止系统创建太多线程，以节省系统资源。

  另外，所设置的系统大小应该能够处理预期的最大吞吐量，而要达到最大吞吐量，系统将需要按照所设置的最大线程数启动所有线程。

  另外，指定一个最小线程数的负面影响非常小，即使第一次就有很多任务运行，不过这种一次性成本负面影响不大。

- 设置额外线程存活时间

  当线程数大于核心线程数时，多余空闲线程在终止前等待新任务的最大存活时间。

  一般而言，一个新线程一旦创建出来，至少应该留存几分钟，以处理任何负载飙升。如果任务达到率有比较好的模型，可以基于这个模型设置空闲时间。另外，空闲时间应该以分钟计，而且至少在10分钟到30分钟之间。

- 选择线程池队列
  - SynchronousQueue

    SynchronousQueue不是一个真正的队列，没法保存任务，它是一种在线程之间进行移交的机制。如果要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接受这个元素。如果没有线程等待，所有线程都在忙碌，并且池中的线程数尚未达到最大，那么ThreadPoolExecutor将创建一个新的线程。否则根据饱和策略，这个任务将被拒绝。

    使用直接移交将更高效，只有当线程池是无界的或者可以拒绝任务时，SynchronousQueue才有实际的价值。在newCachedThreadPool工厂方法中就是用了SynchronousQueue。

  - 无界队列

    如果ThreadPoolExecutor使用的是无界队列，则不会拒绝任何任务。这种情况下，ThreadPoolExecutor最多仅会按最小线程数创建线程，最大线程数被忽略。

    如果最大线程数和最小线程数相同，则这种选择和配置了固定线程数的传统线程池运行机制最为接近，newFixedThreadPool和newSingleThreadExecutor在默认情况下就是使用的一个无界的LinkedBlockingQueue。

  - 有界队列

    一种更稳妥的资源管理策略是使用有界队列，例如ArrayBlockingQueue、有界的LinkedBlockingQueue、PriorityBlockingQueue。

    在有界队列填满之前，最多运行的线程数为设置的核心线程数（最小线程数）。如果队列已满，而又有新任务加进来，并且没有达到最大线程数限制，则会为当前新任务启动一个新线程。如果达到了最大线程数限制，则会根据饱和策略来进行处理。

    一般的，如果线程池较小而队列较大，那么有助于减少内存的使用量，降低CPU的使用率，同时还可以减少上下文切换，但付出的代价是会限制吞吐量。

- 选择合适的饱和策略

  当有界队列被填满后，饱和策略将发挥作用，ThreadPoolExecutor的饱和策略可以通过调用setRejectedExecutionHandler来修改。如果某个任务被提交到一个已关闭的Executor，也会用到饱和策略。JDK提供了几种不同的RejectedExecutionHandler的饱和策略实现：
  - AbortPolicy（中止） 

    该策略是默认的饱和策略；

    会抛出未检查的RejectedExecutionException，调用者可以捕获这个异常，然后根据需求编写自己的处理代码；

  - DiscardPolicy（抛弃） 

    当提交的任务无法保存到队列中等待执行时，Discard策略会悄悄抛弃该任务。

  - DiscardOldestPolicy（抛弃最旧） 

    会抛弃下一个将被执行的任务，然后尝试重新提交的新任务。

    如果工作队列是一个优先队列，那么抛弃最旧的策略，会抛弃优先级最高的任务，因此最好不要将抛弃最旧的饱和策略和优先级队列放在一起使用。

  - CallerRunsPolicy（调用者运行） 

    该策略既不会抛弃任务，也不会抛出异常，而是当线程池中的所有线程都被占用后，并且工作队列被填满后，下一个任务会在调用execute时在主线程中执行，从而降低新任务的流量。由于执行任务需要一定的时间，因此主线程至少在一定的时间内不能提交任何任务，从而使得工作者线程有时间来处理正在执行的任务。

    另一方面，在这期间，主线程不会调用accept，那么到达的请求将被保存在TCP层的队列中而不是在应用程序的队列中。如果持续过载，那么TCP层将最终发现他的请求队列被填满，因此同样会开始抛弃请求。

    当服务器过载时，这种过载情况会逐渐向外蔓延开来——从线程池到工作队列到应用程序再到TCP层，最终到达客户端，导致服务器在高负载的情况下实现一种平缓的性能降低。

  当工作队列被填满后，并没有预定的饱和策略来阻塞execute。因此，可以通过信号量Semaphore来限制任务的到达速率，就可以实现该功能。

    ```
        public class BoundedExecutor {
            private final Executor executor;
            private final Semaphore semaphore;

            public BoundedExecutor(Executor executor, int bound) {
                this.executor = executor;
                this.semaphore = new Semaphore(bound);
            }

            public void submitTask(final Runnable command) throws InterruptedException {
                semaphore.acquire();
                try {
                    executor.execute(command::run);
                } catch (RejectedExecutionException e) {
                    semaphore.release();
                }
            }
        }
    ```

- 选择合适的线程池

  newCachedThreadPool工厂方法是一种很好的默认选择，它能够提供比固定大小的线程池更好的排队性能；

  当需要限制当前任务的数量以满足资源管理器需求时，那么可以选择固定大小的线程池，例如在接受网络请求的服务器程序中，如果不进行限制，那么很容易导致过载问题。

  只有当任务相互独立，为线程池设置界限才合理；如果任务之间存在依赖性，那么有界的线程池或队列就可能导致线程饥饿死锁问题，那么此时应该使用无界的线程池。

  对于提交任务并等待其结果的任务来说，还有一种配置方法，就是使用有界的线程池，并使用SynchronousQueue作为工作队列，以及调用者运行饱和策略。

## 6. 小结

并发编程可以总结为三个核心问题：分工、同步、互斥。

- **分工**：是指如何高效地拆解任务并分配给线程。
- **同步**：是指线程之间如何协作。
- **互斥**：是指保证同一时刻只允许一个线程访问共享资源。



## 面试题

### 1.如果你这样回答“什么是线程安全”，面试官都会对你刮目相看

**不是线程的安全**

面试官问：“什么是线程安全”，如果你不能很好的回答，那就请往下看吧。

论语中有句话叫“学而优则仕”，相信很多人都觉得是“学习好了可以做官”。然而，这样理解却是错的。切记望文生义。

同理，“线程安全”也不是指线程的安全，而是指内存的安全。为什么如此说呢？这和操作系统有关。

目前主流操作系统都是多任务的，即多个进程同时运行。为了保证安全，每个进程只能访问分配给自己的内存空间，而不能访问别的进程的，这是由操作系统保障的。

在每个进程的内存空间中都会有一块特殊的公共区域，通常称为堆（内存）。进程内的所有线程都可以访问到该区域，这就是造成问题的潜在原因。

假设某个线程把数据处理到一半，觉得很累，就去休息了一会，回来准备接着处理，却发现数据已经被修改了，不是自己离开时的样子了。可能被其它线程修改了。

比如把你住的小区看作一个进程，小区里的道路/绿化等就属于公共区域。你拿1万块钱往地上一扔，就回家睡觉去了。睡醒后你打算去把它捡回来，发现钱已经不见了。可能被别人拿走了。

因为公共区域人来人往，你放的东西在没有看管措施时，一定是不安全的。内存中的情况亦然如此。

所以线程安全指的是，在堆内存中的数据由于可以被任何线程访问到，在没有限制的情况下存在被意外修改的风险。

即堆内存空间在没有保护机制的情况下，对多线程来说是不安全的地方，因为你放进去的数据，可能被别的线程“破坏”。

那我们该怎么办呢？解决问题的过程其实就是一个取舍的过程，不同的解决方案有不同的侧重点。



**私有的东西就不该让别人知道**

现实中很多人都会把1万块钱藏着掖着，不让无关的人知道，所以根本不可能扔到大马路上。因为这钱是你的私有物品。

在程序中也是这样的，所以操作系统会为每个线程分配属于它自己的内存空间，通常称为栈内存，其它线程无权访问。这也是由操作系统保障的。

如果一些数据只有某个线程会使用，其它线程不能操作也不需要操作，这些数据就可以放入线程的栈内存中。较为常见的就是局部变量。

```
double avgScore(double[] scores) {
    double sum = 0;
    for (double score : scores) {
        sum += score;
    }
    int count = scores.length;
    double avg = sum / count;
    return avg;
}
```

这里的变量sum，count，avg都是局部变量，它们都会被分配在线程栈内存中。

假如现在A线程来执行这个方法，这些变量会在A的栈内存分配。与此同时，B线程也来执行这个方法，这些变量也会在B的栈内存中分配。

也就是说这些局部变量会在每个线程的栈内存中都分配一份。由于线程的栈内存只能自己访问，所以栈内存中的变量只属于自己，其它线程根本就不知道。

就像每个人的家只属于自己，其他人不能进来。所以你把1万块钱放到家里，其他人是不会知道的。且一般还会放到某个房间里，而不是仍在客厅的桌子上。

所以把自己的东西放到自己的私人地盘，是安全的，因为其他人无法知道。而且越隐私的地方越好。



**大家不要抢，人人有份**

相信聪明的你已经发现，上面的解决方案是基于“位置”的。因为你放东西的“位置”只有你自己知道（或能到达），所以东西是安全的，因此这份安全是由“位置”来保障的。

在程序里就对应于方法的局部变量。局部变量之所以是安全的，就是因为定义它的“位置”是在方法里。这样一来安全是达到了，但是它的使用范围也就被限制在这个方法里了，其它方法想用也不用了啦。

现实中往往会有一个变量需要多个方法都能够使用的情况，此时定义这个变量的“位置”就不能在方法里面了，而应该在方法外面。即从（方法的）局部变量变为（类的）成员变量，其实就是“位置”发生了变化。

那么按照主流编程语言的规定，类的成员变量不能再分配在线程的栈内存中，而应该分配在公共的堆内存中。其实也就是变量在内存中的“位置”发生了变化，由一个私有区域来到了公共区域。因此潜在的安全风险也随之而来。

那怎么保证在公共区域的东西安全呢？答案就是，大家不要抢，人人有份。设想你在街头免费发放矿泉水，来了1万人，你却只有1千瓶水，结果可想而知，一拥而上，场面失守。但如果你有10万瓶水，大家一看，水多着呢，不用着急，一个个排着队来，因为肯定会领到。

东西多了，自然就不值钱了，从另一个角度来说，也就安全了。大街上的共享单车，现在都很安全，因为太多了，到处都是，都长得一样，所以连搞破坏的人都放弃了。因此要让一个东西安全，就疯狂的copy它吧。

回到程序里，要让公共区域堆内存中的数据对于每个线程都是安全的，那就每个线程都拷贝它一份，每个线程只处理自己的这一份拷贝而不去影响别的线程的，这不就安全了嘛。相信你已经猜到了，我要表达的就是ThreadLocal类了。

```
class StudentAssistant {

    ThreadLocal<String> realName = new ThreadLocal<>();
    ThreadLocal<Double> totalScore = new ThreadLocal<>();

    String determineDegree() {
        double score = totalScore.get();
        if (score >= 90) {
            return "A";
        }
        if (score >= 80) {
            return "B";
        }
        if (score >= 70) {
            return "C";
        }
        if (score >= 60) {
            return "D";
        }
        return "E";
    }

    double determineOptionalcourseScore() {
        double score = totalScore.get();
        if (score >= 90) {
            return 10;
        }
        if (score >= 80) {
            return 20;
        }
        if (score >= 70) {
            return 30;
        }
        if (score >= 60) {
            return 40;
        }
        return 60;
    }
}
```

这个学生助手类有两个成员变量，realName和totalScore，都是ThreadLocal类型的。每个线程在运行时都会拷贝一份存储到自己的本地。

A线程运行的是“张三”和“90”，那么这两个数据“张三”和“90”是存储到A线程对象（Thread类的实例对象）的成员变量里去了。假设此时B线程也在运行，是“李四”和“85”，那么“李四”和“85”这两个数据是存储到了B线程对象（Thread类的实例对象）的成员变量里去了。

线程类（Thread）有一个成员变量，类似于Map类型的，专门用于存储ThreadLocal类型的数据。从逻辑从属关系来讲，这些ThreadLocal数据是属于Thread类的成员变量级别的。从所在“位置”的角度来讲，这些ThreadLocal数据是分配在公共区域的堆内存中的。

说的直白一些，就是把堆内存中的一个数据复制N份，每个线程认领1份，同时规定好，每个线程只能玩自己的那份，不准影响别人的。

需要说明的是这N份数据都还是存储在公共区域堆内存里的，经常听到的“线程本地”，是从逻辑从属关系上来讲的，这些数据和线程一一对应，仿佛成了线程自己“领地”的东西了。其实从数据所在“位置”的角度来讲，它们都位于公共的堆内存中，只不过被线程认领了而已。这一点我要特地强调一下。

其实就像大街上的共享单车。原来只有1辆，大家抢着骑，老出问题。现在从这1辆复制出N辆，每人1辆，各骑各的，问题得解。共享单车就是数据，你就是线程。骑行期间，这辆单车从逻辑上来讲是属于你的，从所在位置上来讲还是在大街上这个公共区域的，因为你发现每个小区大门口都贴着“共享单车，禁止入门”。哈哈哈哈。

共享单车是不是和ThreadLocal很像呀。再重申一遍，ThreadLocal就是，把一个数据复制N份，每个线程认领一份，各玩各的，互不影响。



**只能看，不能摸**
放在公共区域的东西，只是存在潜在的安全风险，并不是说一定就不安全。有些东西虽然也在公共区域放着，但也是十分安全的。比如你在大街上放一个上百吨的石头雕像，就非常安全，因为大家都弄不动它。

再比如你去旅游时，经常发现一些珍贵的东西，会被用铁栅栏围起来，上面挂一个牌子，写着“只能看，不能摸”。当然可以国际化一点，“only look，don't touch”。这也是很安全的，因为光看几眼是不可能看坏的。

回到程序里，这种情况就属于，只能读取，不能修改。其实就是常量或只读变量，它们对于多线程是安全的，想改也改不了。

```
class StudentAssistant {
    final double passScore = 60;
}
```

比如把及格分数设定为60分，在前面加上一个final，这样所有线程都动不了它了。这就很安全了。

**小节一下**：以上三种解决方案，其实都是在“耍花招”。

第一种，找个只有自己知道的地方藏起来，当然安全了。

第二种，每人复制1份，各玩各的，互不影响，当然也安全了。

第三种，更狠了，直接规定，只能读取，禁止修改，当然也安全了。

是不是都在“避重就轻”呀。如果这三种方法都解决不了，该怎么办呢？Don't worry，just continue reading。



**没有规则，那就先入为主**

前面给出的三种方案，有点“理想化”了。现实中的情况其实是非常混乱嘈杂的，没有规则的。

比如在中午高峰期你去饭店吃饭，进门后发现只剩一个空桌子了，你心想先去点餐吧，回来就坐这里吧。当你点完餐回来后，发现已经被别人捷足先登了。

因为桌子是属于公共区域的物品，任何人都可以坐，那就只能谁先抢到谁坐。虽然你在人群中曾多看了它一眼，但它并不会记住你容颜。

解决方法就不用我说了吧，让一个人在那儿看着座位，其它人去点餐。这样当别人再来的时候，你就可以理直气壮的说，“不好意思，这个座位，我，已经占了”。

我再次相信聪明的你已经猜到了我要说的东西了，没错，就是（互斥）锁。

回到程序里，如果公共区域（堆内存）的数据，要被多个线程操作时，为了确保数据的安全（或一致）性，需要在数据旁边放一把锁，要想操作数据，先获取锁再说吧。

假设一个线程来到数据跟前一看，发现锁是空闲的，没有人持有。于是它就拿到了这把锁，然后开始操作数据，干了一会活，累了，就去休息了。

这时，又来了一个线程，发现锁被别人持有着，按照规定，它不能操作数据，因为它无法得到这把锁。当然，它可以选择等待，或放弃，转而去干别的。

第一个线程之所以敢大胆的去睡觉，就是因为它手里拿着锁呢，其它线程是不可能操作数据的。当它回来后继续把数据操作完，就可以把锁给释放了。锁再次回到空闲状态，其它线程就可以来抢这把锁了。还是谁先抢到锁谁操作数据。

```
class ClassAssistant {

    double totalScore = 60;
    final Lock lock = new Lock();

    void addScore(double score) {
        lock.obtain();
        totalScore += score;
        lock.release();
    }

    void subScore(double score) {
        lock.obtain();
        totalScore -= score;
        lock.release();
    }
}
```

假定一个班级的初始分数是60分，这个班级抽出10名学生来同时参加10个不同的答题节目，每个学生答对一次为班级加上5分，答错一次减去5分。因为10个学生一起进行，所以这一定是一个并发情形。

因此加分和减分这两个方法被并发的调用，它们共同操作总分数。为了保证数据的一致性，需要在每次操作前先获取锁，操作完成后再释放锁。



**相信世界充满爱，即使被伤害**

再回到一开始的例子，假如你往地上仍1万块钱，是不是一定会丢呢？这要看情况了，如果是在人来人往的都市，可以说肯定会丢的。如果你跑到无人区扔地上，可以说肯定不会丢。

可以看到，都是把东西无保护的放到公共区域里，结果却相差很大。这说明安全问题还和公共区域的环境状况有关系。

比如我把数据放到公共区域的堆内存中，但是始终都只会有1个线程，也就是单线程模型，那这数据肯定是安全的。

再者说，2个线程操作同一个数据和200个线程操作同一个数据，这个数据的安全概率是完全不一样的。肯定线程越多数据不安全的概率越大，线程越少数据不安全的概率越小。取个极限情况，那就是只有1个线程，那不安全概率就是0，也就是安全的。

可能你又猜到了我想表达的内容了，没错，就是CAS。可能大家觉得既然锁可以解决问题，那就用锁得了，为啥又冒出了个CAS呢？

那是因为锁的获取和释放是要花费一定代价的，如果在线程数目特别少的时候，可能根本就不会有别的线程来操作数据，此时你还要获取锁和释放锁，可以说是一种浪费。

针对这种“地广人稀”的情况，专门提出了一种方法，叫CAS（Compare And Swap）。就是在并发很小的情况下，数据被意外修改的概率很低，但是又存在这种可能性，此时就用CAS。

假如一个线程操作数据，干了一半活，累了，想要去休息。（貌似今天的线程体质都不太好）。于是它记录下当前数据的状态（就是数据的值），回家睡觉了。

醒来后打算继续接着干活，但是又担心数据可能被修改了，于是就把睡觉前保存的数据状态拿出来和现在的数据状态比较一下，如果一样，说明自己在睡觉期间，数据没有被人动过（当然也有可能是先被改成了其它，然后又改回来了，这就是ABA问题了），那就接着继续干。如果不一样，说明数据已经被修改了，那之前做的那些操作其实都白瞎了，就干脆放弃，从头再重新开始处理一遍。

所以CAS这种方式适用于并发量不高的情况，也就是数据被意外修改的可能性较小的情况。如果并发量很高的话，你的数据一定会被修改，每次都要放弃，然后从头再来，这样反而花费的代价更大了，还不如直接加锁呢。

这里再解释下ABA问题，假如你睡觉前数据是5，醒来后数据还是5，并不能肯定数据没有被修改过。可能数据先被修改成8然后又改回到5，只是你不知道罢了。对于这个问题，其实也很好解决，再加一个版本号字段就行了，并规定只要修改数据，必须使版本号加1。

这样你睡觉前数据是5版本号是0，醒来后数据是5版本号是0，表明数据没有被修改。如果数据是5版本号是2，表明数据被改动了2次，先改为其它，然后又改回到5。

我再次相信聪明的你已经发现了，这里的CAS其实就是乐观锁，上一种方案里的获取锁和释放锁其实就是悲观锁。乐观锁持乐观态度，就是假设我的数据不会被意外修改，如果修改了，就放弃，从头再来。悲观锁持悲观态度，就是假设我的数据一定会被意外修改，那干脆直接加锁得了。



**作者观点**：

前两种属于隔离法，一个是位置隔离，一个是数据隔离。

然后两种是标记法，一个是只读标记，一个是加锁标记。








[toc]

# MySQL 读写分离、分表分库

## 1.中间件

### 1.1.MyCat

[MyCat 官网](http://mycat.org.cn/)

[你们要的MyCat实现MySQL分库分表来了](https://www.cnblogs.com/fkaka/p/13516475.html)



### 1.2.Sharding-JDBC

[Sharding-JDBC 官网](https://shardingsphere.apache.org/document/legacy/4.x/document/cn/manual/sharding-jdbc/)

[一文快速入门分库分表中间件 Sharding-JDBC （必修课）](https://www.cnblogs.com/chengxy-nds/p/13877422.html)



## 2.分表分库

### 2.1.分库分表带来的复杂性

既然分库分表这么好，那我们是不是在项目初期就应该采用这种方案呢？不要激动，冷静一下，分库分表的确解决了很多问题，但是也给系统带来了很多复杂性，下面简要说一说。

**（1）跨库关联查询**

在单库未拆分表之前，我们可以很方便使用 join 操作关联多张表查询数据，但是经过分库分表后两张表可能都不在一个数据库中，如何使用 join 呢？

有几种方案可以解决：

- 字段冗余：把需要关联的字段放入主表中，避免 join 操作；
- 数据抽象：通过 ETL 等将数据汇合聚集，生成新的表；
- 全局表：比如一些基础表可以在每个数据库中都放一份；
- 应用层组装：将基础数据查出来，通过应用程序计算组装；

**（2）分布式事务**

单数据库可以用本地事务搞定，使用多数据库就只能通过分布式事务解决了。

常用解决方案有：基于可靠消息（MQ）的解决方案、两阶段事务提交、柔性事务等。

**（3）排序、分页、函数计算问题**

在使用 SQL 时 order by、limit 等关键字需要特殊处理，一般来说采用分片的思路：

先在每个分片上执行相应的函数，然后将各个分片的结果集进行汇总和再次计算，最终得到结果。

**（4）分布式 ID**

如果使用 Mysql 数据库在单库单表可以使用 id 自增作为主键，分库分表了之后就不行了，会出现 id 重复。

常用的分布式 ID 解决方案有：

- UUID
- 基于数据库自增单独维护一张 ID表
- 号段模式
- Redis 缓存
- 雪花算法（Snowflake）
- 百度 uid-generator
- 美团 Leaf
- 滴滴 Tinyid

**（5）多数据源**

分库分表之后可能会面临从多个数据库或多个子表中获取数据，一般的解决思路有：客户端适配和代理层适配。

业界常用的中间件有：

- shardingsphere（前身 sharding-jdbc）
- Mycat

### 2.2.分库分表方案

分库分表方案中有常用的方案，hash取模和range范围方案；分库分表方案最主要就是路由算法，把路由的key按照指定的算法进行路由存放。下边来介绍一下两个方案的特点。

#### 2.2.1.hash取模方案

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113151.webp)

在我们设计系统之前，可以先预估一下大概这几年的订单量，如：4000万。每张表我们可以容纳1000万，也我们可以设计4张表进行存储。

> 那具体如何路由存储的呢？hash的方案就是对指定的路由key（如：id）对分表总数进行取模，上图中，id=12的订单，对4进行取模，也就是会得到0，那此订单会放到0表中。id=13的订单，取模得到为1，就会放到1表中。为什么对4取模，是因为分表总数是4。

- 优点：
  - 订单数据可以均匀的放到那4张表中，这样此订单进行操作时，就不会有热点问题。

    > 热点的含义：热点的意思就是对订单进行操作集中到1个表中，其他表的操作很少。

    > 订单有个特点就是时间属性，一般用户操作订单数据，都会集中到这段时间产生的订单。如果这段时间产生的订单 都在同一张订单表中，那就会形成热点，那张表的压力会比较大。

- 缺点：

  - 将来的数据迁移和扩容，会很难。

如：业务发展很好，订单量很大，超出了4000万的量，那我们就需要增加分表数。如果我们增加4个表

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113300.webp)

> 一旦我们增加了分表的总数，取模的基数就会变成8，以前id=12的订单按照此方案就会到4表中查询，但之前的此订单时在0表的，这样就导致了数据查不到。就是因为取模的基数产生了变化。

遇到这个情况，我们小伙伴想到的方案就是做数据迁移，把之前的4000万数据，重新做一个hash方案，放到新的规划分表中。也就是我们要做数据迁移。这个是很痛苦的事情。有些小公司可以接受晚上停机迁移，但大公司是不允许停机做数据迁移的。

> 当然做数据迁移可以结合自己的公司的业务，做一个工具进行，不过也带来了很多工作量，每次扩容都要做数据迁移

那有没有不需要做数据迁移的方案呢，我们看下面的方案

#### 2.2.2.range范围方案

range方案也就是以范围进行拆分数据。

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113322.webp)

range方案比较简单，就是把一定范围内的订单，存放到一个表中；如上图id=12放到0表中，id=1300万的放到1表中。设计这个方案时就是前期把表的范围设计好。通过id进行路由存放。

- 优点

  我们小伙伴们想一下，此方案是不是有利于将来的扩容，不需要做数据迁移。即时再增加4张表，之前的4张表的范围不需要改变，id=12的还是在0表，id=1300万的还是在1表，新增的4张表他们的范围肯定是 大于 4000万之后的范围划分的。

- 缺点

  有热点问题，我们想一下，因为id的值会一直递增变大，那这段时间的订单是不是会一直在某一张表中，如id=1000万 ～ id=2000万之间，这段时间产生的订单是不是都会集中到此张表中，这个就导致1表过热，压力过大，而其他的表没有什么压力。

#### 2.2.3.即不需要迁移数据，又能解决数据热点的问题方案探索

**hash取模方案**：没有热点问题，但扩容迁移数据痛苦

**range方案**：不需要迁移数据，但有热点问题。

那有什么方案可以做到两者的优点结合呢？，**即不需要迁移数据，又能解决数据热点的问题呢？**

其实还有一个现实需求，能否根据服务器的性能以及存储高低，适当均匀调整存储呢？

##### 2.2.3.1.方案思路

**hash是可以解决数据均匀的问题，range可以解决数据迁移问题，那我们可以不可以两者相结合呢？****利用这两者的特性呢？**

我们考虑一下数据的扩容代表着，路由key（如id）的值变大了，这个是一定的，那我们先保证数据变大的时候，**首先用range方案让数据落地到一个范围里面**。这样以后id再变大，**那以前的数据是不需要迁移的**。

但又要考虑到**数据均匀**，那是不是可以在**一定的范围内数据均匀**的呢？因为我们每次的扩容肯定会**事先设计好这次扩容的范围大小**，我们只要**保证这次的范围内的数据均匀**是不是就ok了。

##### 2.2.3.2.方案设计

我们先定义一个group组概念，这组里面包含了一些分库以及分表，如下图

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113447.webp)

上图有几个关键点：

- id=0～4000万肯定落到group01组中
- group01组有3个DB，那一个id如何路由到哪个DB？
- 根据hash取模定位DB，那模数为多少？模数要为所有此group组DB中的表数，上图总表数为10。为什么要去表的总数？而不是DB总数3呢？
- 如id=12，id%10=2；那值为2，落到哪个DB库呢？这是设计是前期设定好的，那怎么设定的呢？
- 一旦设计定位哪个DB后，就需要确定落到DB中的哪张表呢？

##### 2.2.3.3.核心主流程

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113534.webp)

按照上面的流程，我们就可以根据此规则，定位一个id，我们看看有没有避免热点问题。

我们看一下，id在【0，1000万】范围内的，根据上面的流程设计，1000万以内的id都均匀的分配到DB_0,DB_1,DB_2三个数据库中的Table_0表中，为什么可以均匀，因为我们用了hash的方案，对10进行取模。

> 上面我们也提了疑问，为什么对表的总数10取模，而不是DB的总数3进行取模？我们看一下为什么DB_0是4张表，其他两个DB_1是3张表？

在我们安排服务器时，有些服务器的性能高，存储高，就可以安排多存放些数据，有些性能低的就少放点数据。如果我们取模是按照DB总数3，进行取模，那就代表着【0，4000万】的数据是平均分配到3个DB中的，那就不能够实现按照服务器能力适当分配了。

按照Table总数10就能够达到，看如何达到

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113548.webp)

上图中我们对10进行取模，如果值为【0，1，2，3】就路由到DB_0，【4，5，6】路由到DB_1，【7，8，9】路由到DB_2。现在小伙伴们有没有理解，这样的设计就可以把多一点的数据放到DB_0中，其他2个DB数据量就可以少一点。DB_0承担了4/10的数据量，DB_1承担了3/10的数据量，DB_2也承担了3/10的数据量。整个Group01承担了【0，4000万】的数据量。

> 注意：小伙伴千万不要被DB_1或DB_2中table的范围也是0～4000万疑惑了，这个是范围区间，也就是id在哪些范围内，落地到哪个表而已。

上面一大段的介绍，就解决了热点的问题，以及可以按照服务器指标，设计数据量的分配。

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113557.webp)

##### 2.2.3.4.如何扩容

其实上面设计思路理解了，扩容就已经出来了；那就是扩容的时候再设计一个group02组，定义好此group的数据范围就ok了。

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113624.webp)

因为是新增的一个group01组，所以就没有什么数据迁移概念，完全是新增的group组，而且这个group组照样就防止了热点，也就是【4000万，5500万】的数据，都均匀分配到三个DB的table_0表中，【5500万～7000万】数据均匀分配到table_1表中。

##### 2.2.3.5.系统设计

思路确定了，设计是比较简单的，就3张表，把group，DB，table之间建立好关联关系就行了。

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113640.webp)

group和DB的关系

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113645.webp)

table和db的关系

上面的表关联其实是比较简单的，只要原理思路理顺了，就ok了。小伙伴们在开发的时候不要每次都去查询三张关联表，可以保存到缓存中（本地jvm缓存），这样不会影响性能。

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210505113652.webp)

一旦需要扩容，小伙伴是不是要增加一下group02关联关系，那应用服务需要重新启动吗？

简单点的话，就凌晨配置，重启应用服务就行了。但如果是大型公司，是不允许的，因为凌晨也有订单的。那怎么办呢？本地jvm缓存怎么更新呢？

其实方案也很多，可以使用用zookeeper，也可以使用分布式配置，这里是比较推荐使用分布式配置中心的，可以将这些数据配置到分布式配置中心去。

## 3.读写分离



## X.案例分析

### X.1.分库分表之后，id 主键如何处理？

**问：分库分表之后，id 主键如何处理？**

#### X.1.1.面试官心理分析

其实这是分库分表之后你必然要面对的一个问题，就是 id 咋生成？因为要是分成多个表之后，每个表都是从 1 开始累加，那肯定不对啊，需要一个**全局唯一**的 id 来支持。所以这都是你实际生产环境中必须考虑的问题。

#### X.1.2.面试题剖析

**X.1.2.1.基于数据库的实现方案**

**X.1.2.1.1.数据库自增 id**

这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。

这个方案的好处就是方便简单，谁都会用；

**缺点就是单库生成**自增 id，要是高并发的话，就会有瓶颈的；如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值；但是**无论如何都是基于单个数据库**。

**适合的场景**：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你**并发不高，但是数据量太大**导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。

**X.1.2.1.2.设置数据库 sequence 或者表自增字段步长**

可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。

比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。

![图片](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210326123927.png)

**适合的场景**：在用户防止产生的 ID 重复时，这种方案实现起来比较简单，也能达到性能目标。但是服务节点固定，步长也固定，将来如果还要增加服务节点，就不好搞了。

**X.1.2.2.UUID**

好处就是本地生成，不要基于数据库来了；

不好之处就是，UUID 太长了、占用空间大，**作为主键性能太差**了；更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。

适合的场景：如果你是要随机生成个什么文件名、编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。

```
UUID.randomUUID().toString().replace(“-”, “”) -> sfsdf23423rr234sfdaf
```

**X.1.2.3.获取系统当前时间**

这个就是获取当前时间即可，但是问题是，**并发很高的时候**，比如一秒并发几千，**会有重复的情况**，这个是肯定不合适的。基本就不用考虑了。

适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号。

**X.1.2.4.snowflake 算法**

snowflake 算法是 twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。

- 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。

- 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 `2^41 - 1`，也就是可以标识 `2^41 - 1` 个毫秒值，换算成年就是表示69年的时间。

- 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 `2^5`个机房（32个机房），每个机房里可以代表 `2^5` 个机器（32台机器）。

- 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 `2^12 - 1 = 4096`，也就是说可以用这个 12 bit 代表的数字来区分**同一个毫秒内**的 4096 个不同的 id。

```
0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000
```

```
public class IdWorker {

    private long workerId;
    private long datacenterId;
    private long sequence;

    public IdWorker(long workerId, long datacenterId, long sequence) {
        // sanity check for workerId
        // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0
        if (workerId > maxWorkerId || workerId < 0) {
            throw new IllegalArgumentException(
                    String.format("worker Id can't be greater than %d or less than 0", maxWorkerId));
        }
        if (datacenterId > maxDatacenterId || datacenterId < 0) {
            throw new IllegalArgumentException(
                    String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId));
        }
        System.out.printf(
                "worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d",
                timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId);

        this.workerId = workerId;
        this.datacenterId = datacenterId;
        this.sequence = sequence;
    }

    private long twepoch = 1288834974657L;

    private long workerIdBits = 5L;
    private long datacenterIdBits = 5L;

    // 这个是二进制运算，就是 5 bit最多只能有31个数字，也就是说机器id最多只能是32以内
    private long maxWorkerId = -1L ^ (-1L << workerIdBits);

    // 这个是一个意思，就是 5 bit最多只能有31个数字，机房id最多只能是32以内
    private long maxDatacenterId = -1L ^ (-1L << datacenterIdBits);
    private long sequenceBits = 12L;

    private long workerIdShift = sequenceBits;
    private long datacenterIdShift = sequenceBits + workerIdBits;
    private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;
    private long sequenceMask = -1L ^ (-1L << sequenceBits);

    private long lastTimestamp = -1L;

    public long getWorkerId() {
        return workerId;
    }

    public long getDatacenterId() {
        return datacenterId;
    }

    public long getTimestamp() {
        return System.currentTimeMillis();
    }

    public synchronized long nextId() {
        // 这儿就是获取当前时间戳，单位是毫秒
        long timestamp = timeGen();

        if (timestamp < lastTimestamp) {
            System.err.printf("clock is moving backwards.  Rejecting requests until %d.", lastTimestamp);
            throw new RuntimeException(String.format(
                    "Clock moved backwards.  Refusing to generate id for %d milliseconds", lastTimestamp - timestamp));
        }

        if (lastTimestamp == timestamp) {
            // 这个意思是说一个毫秒内最多只能有4096个数字
            // 无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围
            sequence = (sequence + 1) & sequenceMask;
            if (sequence == 0) {
                timestamp = tilNextMillis(lastTimestamp);
            }
        } else {
            sequence = 0;
        }

        // 这儿记录一下最近一次生成id的时间戳，单位是毫秒
        lastTimestamp = timestamp;

        // 这儿就是将时间戳左移，放到 41 bit那儿；
        // 将机房 id左移放到 5 bit那儿；
        // 将机器id左移放到5 bit那儿；将序号放最后12 bit；
        // 最后拼接起来成一个 64 bit的二进制数字，转换成 10 进制就是个 long 型
        return ((timestamp - twepoch) << timestampLeftShift) | (datacenterId << datacenterIdShift)
                | (workerId << workerIdShift) | sequence;
    }

    private long tilNextMillis(long lastTimestamp) {
        long timestamp = timeGen();
        while (timestamp <= lastTimestamp) {
            timestamp = timeGen();
        }
        return timestamp;
    }

    private long timeGen() {
        return System.currentTimeMillis();
    }

    // ---------------测试---------------
    public static void main(String[] args) {
        IdWorker worker = new IdWorker(1, 1, 1);
        for (int i = 0; i < 30; i++) {
            System.out.println(worker.nextId());
        }
    }

}
```

怎么说呢，大概这个意思吧，就是说 41 bit 是当前毫秒单位的一个时间戳，就这意思；然后 5 bit 是你传递进来的一个**机房** id（但是最大只能是 32 以内），另外 5 bit 是你传递进来的**机器** id（但是最大只能是 32 以内），剩下的那个 12 bit序列号，就是如果跟你上次生成 id 的时间还在一个毫秒内，那么会把顺序给你累加，最多在 4096 个序号以内。

所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是 0。然后每次接收到一个请求，说这个机房的这个机器要生成一个 id，你就找到对应的 Worker 生成。

利用这个 snowflake 算法，你可以开发自己公司的服务，甚至对于机房 id 和机器 id，反正给你预留了 5 bit + 5 bit，你换成别的有业务含义的东西也可以的。

这个 snowflake 算法相对来说还是比较靠谱的，所以你要真是搞分布式 id 生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。



### X.2.亿级大表分库分表实战总结

分库分表的文章网上非常多，但是大多内容比较零散，以讲解知识点为主，没有完整地说明一个大表的切分、新架构设计、上线的完整过程。

因此，我结合去年做的一个大型分库分表项目，来复盘一下完整的分库分表从架构设计 到 发布上线的实战总结。

#### X.2.1.前言

为什么需要做分库分表。这个相信大家多少都有所了解。

海量数据的存储和访问成为了MySQL数据库的瓶颈问题，日益增长的业务数据，无疑对MySQL数据库造成了相当大的负载，同时对于系统的稳定性和扩展性提出很高的要求。

而且单台服务器的资源（CPU、磁盘、内存等）总是有限的，最终数据库所能承载的数据量、数据处理能力都将遭遇瓶颈。

目前来说一般有两种方案。

- 一种是更换存储，不使用MySQL，比如可以使用HBase、polarDB、TiDB等分布式存储。

- 如果出于各种原因考虑，还是想继续使用MySQL，一般会采用第二种方式，那就是分库分表。

这里专注于梳理分库分表从架构设计到发布上线的完整过程，同时总结其中的注意事项和最佳实践。包括：

- 业务重构
- 技术架构设计
- 改造和上线
- 稳定性保障
- 项目管理

尤其是各个阶段的最佳实践，都是血与泪凝聚的经验教训。

#### X.2.2.第一阶段：业务重构（可选）

> 对于微服务划分比较合理的分库分表行为，一般只需要关注存储架构的变化，或者只需要在个别应用上进行业务改造即可，一般不需要着重考虑“业务重构” 这一阶段，因此，这一阶段属于“可选”。

本次项目的第一大难点，在于业务重构。

而本次拆分项目涉及到的两张大表A和B，单表将近八千万的数据，是从单体应用时代遗留下来的，从一开始就没有很好的领域驱动/MSA架构设计，逻辑发散非常严重，到现在已经涉及50+个在线服务和20+个离线业务的的直接读写。

因此，如何保证业务改造的彻底性、全面性是重中之重，不能出现有遗漏的情况。

另外，表A 和 表B 各自有二、三十个字段，两表的主键存在一一对应关系，因此，本次分库分表项目中，还需要将两个表进行重构融合，将多余/无用的字段剔除。

##### X.2.2.1 查询统计

在线业务通过分布式链路追踪系统进行查询，按照表名作为查询条件，然后按照服务维度进行聚合，找到所有相关服务，写一个文档记录相关团队和服务。

这里特别注意下，很多表不是只有在线应用在使用，很多离线算法和数据分析的业务也在使用，这里需要一并的梳理好，做好线下跨团队的沟通和调研工作，以免切换后影响正常的数据分析。

##### X.2.2.2 查询拆分与迁移

创建一个jar包，根据2.1的统计结果，与服务owner合作将服务中的相关查询都迁移到这个jar包中（本项目的jar包叫projected），此处为1.0.0-SNAPSHOT版本。

然后将原本服务内的 `xxxMapper.xxxMethod( ) ` 全部改成 `projectdb.xxxMethod( )` 进行调用。

这样做有两个好处：

- 方便做后续的查询拆分分析。
- 方便后续直接将jar包中的查询替换为改造后 中台服务 的rpc调用，业务方只需升级jar包版本，即可快速从sql调用改为rpc查询。

这一步花了几个月的实际，务必梳理各个服务做全面的迁移，不能遗漏，否则可能会导致拆分分析不全面，遗漏了相关字段。

> 查询的迁移主要由于本次拆分项目涉及到的服务太多，需要收拢到一个jar包，更方便后期的改造。如果实际分库分表项目中仅仅涉及一两个服务的，这一步是可以不做的。

##### X.2.2.3 联合查询的拆分分析

根据2.2收拢的jar包中的查询，结合实际情况将查询进行分类和判断，把一些历史遗留的问题，和已经废弃的字段做一些整理。

以下举一些思考点。

1）哪些查询是无法拆分的？例如分页（尽可能地改造，实在改不了只能以冗余列的形式）

2）哪些查询是可以业务上join拆分的？

3）哪些表/字段是可以融合的？

4）哪些字段需要冗余？

5）哪些字段可以直接废弃了？

6）根据业务具体场景和sql整体统计，识别关键的分表键。其余查询走搜索平台。

思考后得到一个查询改造总体思路和方案。

同时在本项目中需要将两张表融合为一张表，废弃冗余字段和无效字段。

##### X.2.2.4 新表设计

这一步基于2.3对于查询的拆分分析，得出旧表融合、冗余、废弃字段的结果，设计新表的字段。

产出新表设计结构后，必须发给各个相关业务方进行review，并保证所有业务方都通过该表的设计。有必要的话可以进行一次线下review。

如果新表的过程中，对部分字段进行了废弃，必须通知所有业务方进行确认。

对于新表的设计，除了字段的梳理，也需要根据具体查询，重新设计、优化索引。

##### X.2.2.5 第一次升级

新表设计完成后，先做一次jar包内sql查询的改造，将旧的字段全部更新为新表的字段。此处为2.0.0-SNAPSHOT版本。

然后让所有服务升级jar包版本，以此来保证这些废弃字段确实是不使用了，新的表结构字段能够完全覆盖过去的业务场景。

特别注意的是，由于涉及服务众多，可以将服务按照 非核心 与 核心 区分，然后分批次上线，避免出现问题导致严重故障或者大范围回滚。

##### X.2.2.6 最佳实践

**X.2.2.6.1 尽量不改变原表的字段名称**

在做新表融合的时候，一开始只是简单归并表A 和 表B的表，因此很多字段名相同的字段做了重命名。

后来字段精简过程中，删除了很多重复字段，但是没有将重命名的字段改回来。

导致后期上线的过程中，不可避免地需要业务方进行重构字段名。

因此，新表设计的时候，除非必不得已，不要修改原表的字段名称！

**X.2.2.6.2 新表的索引需要仔细斟酌**

新表的索引不能简单照搬旧表，而是需要根据查询拆分分析后，重新设计。

尤其是一些字段的融合后，可能可以归并一些索引，或者设计一些更高性能的索引。

##### X.2.2.7 本章小结

至此，分库分表的第一阶段告一段落。这一阶段所需时间，完全取决于具体业务，如果是一个历史包袱沉重的业务，那可能需要花费几个月甚至半年的时间才能完成。

这一阶段的完成质量非常重要，否则可能导致项目后期需要重建表结构、重新全量数据。

> 这里再次说明，对于微服务划分比较合理的服务，分库分表行为一般只需要关注存储架构的变化，或者只需要在个别应用上进行业务改造即可，一般不需要着重考虑“业务重构” 这一阶段。

 

#### X.2.3.第二阶段：存储架构设计（核心）

对于任何分库分表的项目，存储架构的设计都是最核心的部分！

##### X.2.3.1 整体架构

根据第一阶段整理的查询梳理结果，我们总结了这样的查询规律。

- 80%以上的查询都是通过或者带有字段pk1、字段pk2、字段pk3这三个维度进行查询的，其中pk1和pk2由于历史原因存在一一对应的关系
- 20%的查询千奇百怪，包括模糊查询、其他字段查询等等

因此，我们设计了如下的整体架构，引入了数据库中间件、数据同步工具、搜索引擎（阿里云opensearch/ES）等。

下文的论述都是围绕这个架构来展开的。

![922dc579751f4475b83faaf1fde7c876](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210313102126.png)

**X.2.3.1.1 mysql分表存储**

Mysql分表的维度是根据查询拆分分析的结果确定的。

我们发现pk1\pk2\pk3可以覆盖80%以上的主要查询。让这些查询根据分表键直接走mysql数据库即可。

原则上一般最多维护一个分表的全量数据，因为过多的全量数据会造成存储的浪费、数据同步的额外开销、更多的不稳定性、不易扩展等问题。

但是由于本项目pk1和pk3的查询语句都对实时性有比较高的要求，因此，维护了pk1和pk3作为分表键的两份全量数据。

而pk2和pk1由于历史原因，存在一一对应关系，可以仅保留一份映射表即可，只存储pk1和pk2两个字段。

**X.2.3.1.2 搜索平台索引存储**

搜索平台索引，可以覆盖剩余20%的零散查询。

这些查询往往不是根据分表键进行的，或者是带有模糊查询的要求。

对于搜索平台来说，一般不存储全量数据（尤其是一些大varchar字段），只存储主键和查询需要的索引字段，搜索得到结果后，根据主键去mysql存储中拿到需要的记录。

当然，从后期实践结果来看，这里还是需要做一些权衡的：

- 有些非索引字段，如果不是很大，也可以冗余进来，类似覆盖索引，避免多一次sql查询；

- 如果表结构比较简单，字段不大，甚至可以考虑全量存储，提高查询性能，降低mysql数据库的压力。

> 这里特别提示，搜索引擎和数据库之间同步是必然存在延迟的。所以对于根据分表id查询的语句，尽量保证直接查询数据库，这样不会带来一致性问题的隐患。

**X.2.3.1.3 数据同步**

一般新表和旧表直接可以采用 数据同步 或者 双写的方式进行处理，两种方式有各自的优缺点。

![8e4e74a079654f0e9e12ebdd359d0296](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210313102149.jpg)

一般根据具体情况选择一种方式就行。

本次项目的具体同步关系见整体存储架构，包括了四个部分：

- 1）旧表到新表全量主表的同步

  一开始为了减少代码入侵、方便扩展，采用了数据同步的方式。而且由于业务过多，担心有未统计到的服务没有及时改造，所以数据同步能避免这些情况导致数据丢失。

  但是在上线过程中发现，当延迟存在时，很多新写入的记录无法读到，对具体业务场景造成了比较严重的影响。（具体原因参考4.5.1的说明）

  因此，为了满足应用对于实时性的要求，我们在数据同步的基础上，重新在3.0.0-SNAPSHOT版本中改造成了双写的形式。

- 2）新表全量主表到全量副表的同步

- 3）新表全量主表到映射表到同步

- 4）新表全量主表到搜索引擎数据源的同步

2）、3）、4）都是从新表全量主表到其他数据源的数据同步，因为没有强实时性的要求，因此，为了方便扩展，全部采用了数据同步的方式，没有进行更多的多写操作。

##### X.2.3.2 容量评估

在申请mysql存储和搜索平台索引资源前，需要进行容量评估，包括存储容量和性能指标。

具体线上流量评估可以通过监控系统查看qps，存储容量可以简单认为是线上各个表存储容量的和。

但是在全量同步过程中，我们发现需要的实际容量的需求会大于预估，具体可以看3.4.6的说明。

具体性能压测过程就不再赘述。

##### X.2.3.3 数据校验

从上文可以看到，在本次项目中，存在大量的业务改造，属于异构迁移。

从过去的一些分库分表项目来说，大多是同构/对等拆分，因此不会存在很多复杂逻辑，所以对于数据迁移的校验往往比较忽视。

在完全对等迁移的情况下，一般确实比较少出现问题。

但是，类似这样有比较多改造的异构迁移，校验绝对是重中之重！！

因此，必须对数据同步的结果做校验，保证业务逻辑改造正确、数据同步一致性正确。这一点非常非常重要。

在本次项目中，存在大量业务逻辑优化以及字段变动，所以我们单独做了一个校验服务，对数据的全量、增量进行校验。

过程中提前发现了许多数据同步、业务逻辑的不一致问题，给我们本次项目平稳上线提供了最重要的前提保障！！

##### X.2.3.4 最佳实践

**X.2.3.4.1 分库分表引起的流量放大问题**

在做容量评估的时候，需要关注一个重要问题。就是分表带来的查询流量放大。

这个流量放大有两方面的原因：

- 索引表的二次查询。比如根据pk2查询的，需要先通过pk2查询pk1，然后根据pk1查询返回结果。
- in的分批查询。如果一个select...in...的查询，数据库中间件会根据分表键，将查询拆分落到对应的物理分表上，相当于原本的一次查询，放大为多次查询。（当然，数据库会将落在同一个分表的id作为一次批量查询，而这是不稳定的合并）

因此，我们需要注意：

- 业务层面尽量限制in查询数量，避免流量过于放大；
- 容量评估时，需要考虑这部分放大因素，做适当冗余，另外，后续会提到业务改造上线分批进行，保证可以及时扩容；
- 分64、128还是256张表有个合理预估，拆得越多，理论上会放大越多，因此不要无谓地分过多的表，根据业务规模做适当估计；
- 对于映射表的查询，由于存在明显的冷热数据，所以我们又在中间加了一层缓存，减少数据库的压力

**X.2.3.4.2 分表键的变更方案**

本项目中，存在一种业务情况会变更字段pk3，但是pk3作为分表键，在数据库中间件中是不能修改的，因此，只能在中台中修改对pk3的更新逻辑，采用先删除、后添加的方式。

这里需要注意，删除和添加操作的事务原子性。当然，简单处理也可以通过日志的方式，进行告警和校准。

**X.2.3.4.3 数据同步一致性问题**

我们都知道，数据同步中一个关键点就是（消息）数据的顺序性，如果不能保证接受的数据和产生的数据的顺序严格一致，就有可能因为（消息）数据乱序带来数据覆盖，最终带来不一致问题。

我们自研的数据同步工具底层使用的消息队列是kakfa，，kafka对于消息的存储，只能做到局部有序性（具体来说是每一个partition的有序）。我们可以把同一主键的消息路由至同一分区，这样一致性一般可以保证。但是，如果存在一对多的关系，就无法保证每一行变更有序，见如下例子。

![e206b0ead22641a4ab1265d54a1394ce](https://homan-blog.oss-cn-beijing.aliyuncs.com/study-demo/mysql-demo/20210313102207.png)

那么需要通过反查数据源获取最新数据保证一致性。

但是，反查也不是“银弹“，需要考虑两个问题。

1）如果消息变更来源于读写实例，而反查 数据库是查只读实例，那就会存在读写实例延迟导致的数据不一致问题。因此，需要保证 消息变更来源 和 反查数据库 的实例是同一个。

2）反查对数据库会带来额外性能开销，需要仔细评估全量时候的影响。

**X.2.3.4.4 数据实时性问题**

延迟主要需要注意几方面的问题，并根据业务实际情况做评估和衡量。

1）数据同步平台的秒级延迟

2）如果消息订阅和反查数据库都是落在只读实例上，那么除了上述数据同步平台的秒级延迟，还会有数据库主从同步的延迟

3）宽表到搜索平台的秒级延迟

只有能够满足业务场景的方案，才是合适的方案。

**X.2.3.4.5 分表后存储容量优化**

由于数据同步过程中，对于单表而言，不是严格按照递增插入的，因此会产生很多”存储空洞“，使得同步完后的存储总量远大于预估的容量。

因此，在新库申请的时候，存储容量多申请50%。

##### X.2.3.5 本章小结

至此，分库分表的第二阶段告一段落。

这一阶段踩了非常多的坑。

一方面是设计高可用、易扩展的存储架构。在项目进展过程中，也做了多次的修改与讨论，包括mysql数据冗余数量、搜索平台的索引设计、流量放大、分表键修改等问题。

另一方面是“数据同步”本身是一个非常复杂的操作，正如本章最佳实践中提及的实时性、一致性、一对多等问题，需要引起高度重视。

因此，更加依赖于数据校验对最终业务逻辑正确、数据同步正确的检验！

在完成这一阶段后，可以正式进入业务切换的阶段。需要注意的是，数据校验仍然会在下一阶段发挥关键性作用。

#### X.2.4.第三阶段：改造和上线（慎重）

前两个阶段完成后，开始业务切换流程，主要步骤如下：

1）中台服务采用单读 双写 的模式

2）旧表往新表开着数据同步

3） 所有服务升级依赖的projectDB版本，上线RPC，如果出现问题，降版本即可回滚（上线成功后，单读新库，双写新旧库）

4）检查监控确保没有 中台服务 以外的其他服务访问旧库旧表

5）停止数据同步

6）删除旧表

##### X.2.4.1 查询改造

如何验证我们前两个阶段设计是否合理？能否完全覆盖查询的修改 是一个前提条件。

当新表设计完毕后，就可以以新表为标准，修改老的查询。

以本项目为例，需要将旧的sql在 新的中台服务中 进行改造。

1）读查询的改造

可能查询会涉及以下几个方面：

a）根据查询条件，需要将pk1和pk2的inner join改为对应分表键的新表表名

b）部分sql的废弃字段处理

c）非分表键查询改为走搜索平台的查询，注意保证语义一致

d）注意写单测避免低级错误，主要是DAO层面。

只有新表结构和存储架构能完全适应查询改造，才能认为前面的设计暂时没有问题。

当然，这里还有个前提条件，就是相关查询已经全部收拢，没有遗漏。

2) 写查询的改造

除了相关字段的更改以外，更重要的是，需要改造为旧表、新表的双写模式。

这里可能涉及到具体业务写入逻辑，本项目尤为复杂，需要改造过程中与业务方充分沟通，保证写入逻辑正确。

可以在双写上各加一个配置开关，方便切换。如果双写中发现新库写入有问题，可以快速关闭。

同时，双写过程中不关闭 旧库到新库 的数据同步。

为什么呢？主要还是由于我们项目的特殊性。由于我们涉及到几十个服务，为了降低风险，必须分批上线。因此，存在比较麻烦的中间态，一部分服务是老逻辑，一部分服务是新逻辑，必须保证中间态的数据正确性，具体见4.5.1的分析。

##### X.2.4.2 服务化改造

为什么需要新建一个 服务来 承载改造后的查询呢？

一方面是为了改造能够方便的升级与回滚切换，另一方面是为了将查询收拢，作为一个中台化的服务来提供相应的查询能力。

将改造后的新的查询放在服务中，然后jar包中的原本查询，全部替换成这个服务的client调用。

同时，升级jar包版本到3.0.0-SNAPSHOT。

##### X.2.4.3 服务分批上线

为了降低风险，需要安排从非核心服务到核心服务的分批上线。

注意，分批上线过程中，由于写服务往往是核心服务，所以安排在后面。可能出现非核心的读服务上线了，这时候会有读新表、写旧表的中间状态。

1） 所有相关服务使用 重构分支 升级projectdb版本到3.0.0-SNAPSHOT并部署内网环境；

2） 业务服务依赖于 中台服务，需要订阅服务

3） 开重构分支（不要与正常迭代分支合并），部署内网，内网预计测试两周以上

使用一个新的 重构分支 是为了在内网测试两周的时候，不影响业务正常迭代。每周更新的业务分支可以merge到重构分支上部署内网，然后外网使用业务分支merge到master上部署。

当然，如果从线上线下代码分支一致的角度，也可以重构分支和业务分支一起测试上线，对开发和测试的压力会较大。

4）分批上线过程中，如果碰到依赖冲突的问题，需要及时解决并及时更新到该文档中

5）服务上线前，必须要求业务开发或者测试，明确评估具体api和风险点，做好回归。

这里再次提醒，上线完成后，请不要漏掉离线的数据分析业务！请不要漏掉离线的数据分析业务！请不要漏掉离线的数据分析业务！

##### 4.4 旧表下线流程

1）检查监控确保没有中台服务以外的其他服务访问旧库旧表

2）检查数据库上的sql审计，确保没有其他服务仍然读取旧表数据

3）停止数据同步

4）删除旧表

##### X.2.4.5 最佳实践

**X.2.4.5.1 写完立即读可能读不到**

在分批上线过程中，遇到了写完立即读可能读不到的情况。由于业务众多，我们采用了分批上线的方式降低风险，存在一部分应用已经升级，一部分应用尚未升级的情况。未升级的服务仍然往旧表写数据，而升级后的应用会从新表读数据，当延迟存在时，很多新写入的记录无法读到，对具体业务场景造成了比较严重的影响。

延迟的原因主要有两个：

1）写服务还没有升级，还没有开始双写，还是写旧表，这时候会有读新表、写旧表的中间状态，新旧表存在同步延迟。

2）为了避免主库压力，新表数据是从旧表获取变更、然后反查旧表只读实例的数据进行同步的，主从库本身存在一定延迟。

解决方案一般有两种：

1）数据同步改为双写逻辑。

2）在读接口做补偿，如果新表查不到，到旧表再查一次。

**X.2.4.5.2 数据库中间件唯一ID替换自增主键（划重点，敲黑板）**

由于分表后，继续使用单表的自增主键，会导致全局主键冲突。因此，需要使用分布式唯一ID来代替自增主键。各种算法网上比较多，本项目采用的是数据库自增sequence生成方式。

> 数据库自增sequence的分布式ID生成器，是一个依赖Mysql的存在， 它的基本原理是在Mysql中存入一个数值， 每有一台机器去获取ID的时候，都会在当前ID上累加一定的数量比如说2000， 然后把当前的值加上2000返回给服务器。这样每一台机器都可以继续重复此操作获得唯一id区间。

但是仅仅有全局唯一ID就大功告成了吗？显然不是，因为这里还会存在新旧表的id冲突问题。

因为服务比较多，为了降低风险需要分批上线。因此，存在一部分服务还是单写旧表的逻辑，一部分服务是双写的逻辑。

这样的状态中，旧表的id策略使用的是auto_increment。如果只有单向数据来往的话（旧表到新表），只需要给旧表的id预留一个区间段，sequence从一个较大的起始值开始就能避免冲突。

但该项目中，还有新表数据和旧表数据的双写，如果采用上述方案，较大的id写入到旧表，旧表的auto_increment将会被重置到该值，这样单鞋旧表的服务产生的递增id的记录必然会出现冲突。

所以这里交换了双方的区间段，旧库从较大的auto_increment起始值开始，新表选择的id（也就是sequence的范围）从大于旧表的最大记录的id开始递增，小于旧表auto_increment即将设置的起始值，很好的避免了id冲突问题。

1）切换前：

sequence的起始id设置为当前旧表的自增id大小，然后旧表的自增id需要改大，预留一段区间，给旧表的自增id继续使用，防止未升级业务写入旧表的数据同步到新库后产生id冲突；

2）切换后

无需任何改造，断开数据同步即可

3）优点

只用一份代码；

切换可以使用开关进行，不用升级改造；

如果万一中途旧表的autoincrement被异常数据变大了，也不会造成什么问题。

4）缺点

如果旧表写失败了，新表写成功了，需要日志辅助处理

##### X.2.4.6 本章小结

完成旧表下线后，整个分库分表的改造就完成了。

在这个过程中，需要始终保持对线上业务的敬畏，仔细思考每个可能发生的问题，想好快速回滚方案（在三个阶段提到了projectdb的jar包版本迭代，从1.0.0-SNAPSHOT到3.0.0-SNAPSHOT，包含了每个阶段不同的变更，在不同阶段的分批上线的过程中，通过jar包版本的方式进行回滚，发挥了巨大作用），避免造成重大故障。

#### X.2.5.稳定性保障

这一章主要再次强调稳定性的保障手段。作为本次项目的重要目标之一，稳定性其实贯穿在整个项目周期内，基本上在上文各个环节都已经都有提到，每一个环节都要引起足够的重视，仔细设计和评估方案，做到心中有数，而不是靠天吃饭：

1）新表设计必须跟业务方充分沟通、保证review。

2）对于“数据同步”，必须有数据校验保障数据正确性，可能导致数据不正确的原因上文已经提到来很多，包括实时性、一致性的问题。保证数据正确是上线的大前提。

3）每一阶段的变动，都必须做好快速回滚都预案。

4）上线过程，都以分批上线的形式，从非核心业务开始做试点，避免故障扩大。

5）监控告警要配置全面，出现问题及时收到告警，快速响应。不要忽略，很重要，有几次出现选过数据的小问题，都是通过告警及时发现和解决的

6）单测，业务功能测试等要充分

#### X.2.6.项目管理之跨团队协作

关于“跨团队协作”，本文专门拎出来作为一章。

因为在这样一个跨团队的大型项目改造过程中，科学的团队协作是保障整体项目按时、高质量完成的不可缺少的因素。

下面，分享几点心得与体会。

##### X.2.6.1 一切文档先行

团队协作最忌“空口无凭”。

无论是团队分工、进度安排或是任何需要多人协作的事情，都需要有一个文档记录，用于追踪进度，把控流程。

##### X.2.6.2 业务沟通与确认

所有的表结构改造，必须跟相关业务方沟通，对于可能存在的历史逻辑，进行全面梳理；

所有讨论确定后的字段改造，必须由每个服务的Owner进行确认。

##### X.2.6.3 责任到位

对于多团队多人次的合作项目，每个团队都应该明确一个对接人，由项目总负责人与团队唯一对接人沟通，明确团队完整进度和完成质量。

#### X.2.7.展望

其实，从全文的篇幅就能够看出，本次的分库分表项目由于复杂的业务逻辑改造，费大量的时间和精力，并且非常容易在改造过程中，引起不稳定的线上问题。

本文复盘了整个分库分表从拆分、设计、上线的整体过程，希望能对大家有所帮助。

 

看到这里，我们会想问一句。所以，有没有更好的方式呢？

也许，未来还是需要去结合业界新的数据库中间件技术，能够快速实现分库分表。

也许，未来还可以引入新的数据存储技术与方案（polardb、tidb、hbase），根本不再需要分库分表呢？

继续跟进新技术的发展，我相信会找到答案。

















